# 项目概览：ToxGIN、EviDTI 与相关参考项目

本文档基于对 ToxGIN、EviDTI 的阅读与梳理，汇总其研究目标、数据与特征形态、核心模型结构、训练/推理流程、产出物与复现注意事项；并在后续章节补充若干相关项目的可借鉴点，便于后续在毒性/药物发现相关工作中快速对齐思路与复用组件。

- ToxGIN：`/root/private_data/dd_model/ToxGIN`
- EviDTI：`/root/private_data/dd_model/EviDTI`

---

## 1. ToxGIN（peptide toxicity prediction with GIN + ESM2 + 3D graph）

### 1.1 任务定义与输入输出

- **任务**：二分类（或概率预测）——预测给定肽（peptide）是否具有毒性，输出毒性概率。
- **输入**：
  - 肽序列（CSV 中 `sequence` 列；样例：`train_sequence.csv`、`test_sequence.csv`）。
  - 肽的 3D 结构文件（PDB），文件名约定为：`{sequence}.pdb`（见 README 说明）。
    - 结构来源：论文/README 描述为通过 ColabFold 预测得到的肽结构。
- **输出**：
  - 训练：保存最优模型权重（`*.pth`）与训练日志（CSV + TensorBoard）。
  - 推理：为每条序列输出 `predicted_probability`（见 `predict.py` 写出的 `predictions.csv`）。

### 1.2 方法概述（从“结构图”到“毒性概率”）

ToxGIN 的核心思想是把肽的 3D 结构转成图（residue-level graph），并将序列语言模型表示（ESM2）与理化性质特征（AAindex1）融合到节点特征中，再用 GIN 聚合得到图级表征进行分类：

1. **3D 结构 → 图**
   - 节点：氨基酸残基（通过 PDB 的 `residue_number` 聚合原子坐标得到残基坐标）。
   - 边：残基间距离阈值连接（默认阈值 `8.0Å`，`distance < threshold` 建边），构成 DGLGraph（见 `preprocess.py` 的 `generate_graph`）。
2. **序列/残基特征**
   - **ESM2**：对残基序列做 tokenization 后取每个残基的 embedding（`facebook/esm2_t36_3B_UR50D`；见 `preprocess.py` 的 `seq_encode`）。
   - **AAindex1**：对每个氨基酸读取 AAindex1 理化描述符（`aaindex1.csv`），并做 MinMaxScaler 归一化（见 `extract_features` + `MinMaxScaler`）。
   - **拼接**：每个残基节点特征为 `[ESM2_embedding ; normalized_AAindex1]`。
3. **GIN 编码 + MLP 分类**
   - 先将节点特征线性映射到隐藏维度，再经过多层 GINConv 聚合。
   - 图级 readout 用 `sum` 聚合（`dgl.readout_nodes(op='sum')`）。
   - 多层 MLP + Sigmoid 输出毒性概率（见 `train.py` 的 `GNNModel`）。

### 1.3 数据与预处理（代码与文件组织）

仓库内提供了**样例序列 CSV**与**脚本**，但训练/测试结构与完整训练数据通常需要下载（README 里提供 Google Drive 链接）。

- 预处理脚本：`/root/private_data/dd_model/ToxGIN/preprocess.py`
  - 读取：`train_sequence.csv`、`test_sequence.csv` + 对应 `*_structures/{sequence}.pdb`
  - 生成：
    - `train_data/`、`test_data/`：每条序列一个 `npz`（键名 `wildtype_seq`，shape 约为 `[L, 3113]`，其中 `3113 = 2560(ESM2) + 553(AAindex1)`）
    - `dgl_graph.bin`：按样本顺序保存的图列表
- 推理预处理脚本：`/root/private_data/dd_model/ToxGIN/predict_preprocess.py`（逻辑与上面类似，但只处理测试集）

**注意点（影响可复现/可运行性）**

- `seq_encode` 中对输入 tensor 使用了 `.cuda()`，在无 GPU 环境下会报错（即使 `device` 检测为 CPU）。若在 CPU 上跑需要改为 `.to(device)`。
- `max_length=60` 且 `truncation=True`：序列长度超过 60 会被截断（会影响长肽的表示）。
- 每条样本都对 AAindex 做 `fit_transform`（按样本独立归一化），这会导致不同样本尺度不一致；这是代码当前行为，复现论文结果时应保持一致。

### 1.4 模型结构与训练策略（从代码提取的关键信息）

核心实现位于：`/root/private_data/dd_model/ToxGIN/train.py`

- **图编码器**：4 层 GINConv（每层含 2 层 Linear + BN + LeakyReLU），并拼接所有层输出形成多尺度表征（`out_dim = hidden_dim * (layer_num+1)`）。
- **分类头**：多层全连接 + Dropout（`gcn.out_dim → 3000 → 2000 → 1000 → 1`）+ Sigmoid。
- **训练**：
  - 5-fold StratifiedKFold（`n_splits=5`）
  - `epochs=500`，`batch_size=256`，`lr=1e-4`，`weight_decay=1e-2`
  - 学习率：`ExponentialLR(gamma=0.95)`（每 epoch step）
  - 指标：AUROC、AUPRC、Accuracy、F1、MCC、Sensitivity、Specificity、FDR（写入 CSV 并记录到 TensorBoard）
  - early stopping：基于 MCC（`patience=50`）并保存 `best_model{cv}.pth`
  - 数据增强：每个 batch **总是**随机删除一部分节点（`random.random() < 1`），删除数量约为 `1 ~ num_nodes/10`（对 batched graph 全局随机采样节点）

### 1.5 推理流程（离线批量预测）

核心实现位于：`/root/private_data/dd_model/ToxGIN/predict.py`

1. 先运行推理预处理脚本生成 `test_data/` 与 `test_data/dgl_graph.bin`。
2. 加载 `best_model.pth`（权重文件）并对批量图做预测。
3. 输出 `predictions.csv`，包含每条序列的 `predicted_probability`。

### 1.6 数据与资源链接（仓库内给出的下载入口）

以下目录的 `README.md` 给出了数据下载链接（Google Drive）：

- `model/README.md`
- `train_dataset/README.md`
- `test_dataset/README.md`
- `train_structures/README.md`
- `test_structures/README.md`

### 1.7 论文与实现对应关系

- 论文链接（仓库 README 提供）：*ToxGIN: an In silico prediction model for peptide toxicity via graph isomorphism networks integrating peptide sequence and structure information*（Briefings in Bioinformatics, 2024）。
- 代码与论文主线对应：结构图构建 + ESM2 残基表示 + AAindex 特征融合 + GIN 图级分类。

---

## 2. EviDTI（Evidential Deep Learning for guided DTI prediction）

### 2.1 任务定义与输入输出

- **任务**：药物-靶点相互作用（DTI）预测，代码实现形式为**二分类**（label 为 0/1），同时输出**不确定性/置信度**相关量（evidential learning）。
- **输入（按样本）**：
  - 靶点蛋白序列特征（`t_1D_feature`）：来自预训练蛋白语言模型的 residue-level embedding（配置默认 `INPUT_DIM=1024`）。
  - 药物 2D 特征（`d_2D_feature`）：一维向量/序列特征（后续经 1D-CNN + MLP）。
  - 药物 3D 结构图特征（`d_3D_feature`）：原子/键图 + “BondAngleGraph”（见 `function.py` 的 `bag` 字段）。
  - 元数据（`metadata`）：`uid/cid/seq/smiles/length` 等，用于 mask 与结果写出。
- **输出**：
  - 训练：最优 checkpoint 保存到 `runs/.../checkpoint.pth`（直接 `torch.save(self.model, ...)` 保存整个模型对象）。
  - 推理：输出预测概率（从 Dirichlet 参数归一化得到）以及 evidential 不确定性（如 `conf = K / sum(alpha)`），并可保存为 CSV（见 `infernce.py` / Solver 的 `result.csv`）。

### 2.2 核心思想：用“证据（evidence）”表达预测与不确定性

EviDTI 的关键在于将分类输出从“logit/概率”改为 **Dirichlet 分布参数** `alpha`：

- 模型输出：`alpha = softplus(logits) + 1`，保证 `alpha > 1`（见 `drugbank_model.py` 等的 `output = softplus(predict) + 1`）。
- 概率（期望）：`p = alpha / sum(alpha)`。
- 不确定性/置信度（常见 evidential 指标之一）：`conf = K / sum(alpha)`（`K=2` 类别数），`sum(alpha)` 越小通常表示证据越少、不确定性越大（见 `drugbank_solver.py: predict_val`）。
- 损失：使用 Dirichlet evidential loss（Sensoy et al. 风格），由“误差项（SOS）+ KL 正则”组成（`utils.py: dirichlet_loss`）。

### 2.3 数据组织与样本构建（PyTorch Geometric InMemoryDataset）

核心实现位于：`/root/private_data/dd_model/EviDTI/function.py`

以 `bond_angle_graph_data`（DrugBank 场景）为例，`process()` 会将每条 DTI 样本组织为一个 `torch_geometric.data.Data`：

- `Data.x`：原子节点特征（由 7 类原子离散属性堆叠得到）
  - `atomic_num/chiral_tag/degree/explicit_valence/formal_charge/hybridization/implicit_valence`
- `Data.edge_index`：原子图边
- `Data.edge_attr`：键特征（3 类离散 + 1 个连续：`bond_length`）
- `Data.bag`：BondAngleGraph（以“键”为中心的二级图），其 `edge_attr` 为 `bond_angle`
- `Data.d_2D_feature`：药物 2D 特征（从 `d_2D_path` 的 `.npy` 读取）
- `Data.t_1D_feature`：蛋白序列特征（从 `t_1D_path` 的 `.npy` 读取）
- `Data.l`：label（0/1）
- `Data.metadata`：字典（含 `uid/cid/seq/smiles/length`）

同类实现还包括：`KIBA_graph_data`、`davis_graph_data`、`case_graph_data`（处理不同数据集与字段差异）。

### 2.4 模型结构（LightAttention：蛋白 LA + 药物 2D-CNN + 药物 3D 图）

以 DrugBank 版本为例：`/root/private_data/dd_model/EviDTI/drugbank_model.py: LightAttention`

1. **蛋白分支（Light Attention）**
   - 输入：`t_1D_feature`（变长 residue embedding，默认 1024 维）
   - 用 `pad_sequence` 补齐后构造 `mask`，并通过两路 1D-CNN 得到：
     - feature map：`feature_convolution(t_1D)`
     - attention map：`attention_convolution(t_1D)`（对 padding 位置 mask 为 `-1e9`）
   - 聚合：`sum(feature * softmax(attention))` 与 `max(feature)` 拼接，再经 MLP 得到固定维度表示。
2. **药物 2D 分支（1D-CNN + MLP）**
   - `d_2D_feature` reshape 成 `[B,1,L]`，经两层 Conv1d+MaxPool1d 后展平，再经两层全连接得到表示。
3. **药物 3D 分支（原子图 + BondAngleGraph 的耦合消息传递）**
   - 原子/键离散特征用 embedding（`AtomEmbedding/BondEmbedding`），连续键长/键角用 RBF 映射（`BondFloatRBF/BondAngleFloatRBF`）。
   - 通过多层 GATConv 在原子图与 BondAngleGraph 之间交替更新（代码中体现为 `abg_conv*` 与 `bag_conv*` 迭代）。
   - 图级 readout：`global_max_pool`。
4. **融合与 evidential 输出**
   - 将蛋白表示、药物 2D 表示、药物 3D 表示拼接，经多层 MLP 输出 `alpha`（2 维）。

备注：Davis 与 KIBA 的 `LightAttention` 实现（`davis_model.py`、`d_2D_cnn_model.py`）结构与 DrugBank 版本整体一致，主要是层数/超参略有差别。

### 2.5 训练流程（main 脚本 + Solver）

- 入口脚本：
  - `main_drugbank.py`：`python main_drugbank.py --cfg configs/drugbank.yaml --data drugbank --split random`
  - `main_davis.py`：`python main_davis.py --cfg configs/davis.yaml --data davis --split random`
  - `main_KIBA.py`：`python main_KIBA.py --cfg configs/kiba.yaml --data kiba --split random`
- 数据划分：代码中使用固定随机种子后 `np.random.permutation` 做 `80/10/10` 切分，并用 `SubsetRandomSampler` 构造 DataLoader。
- 训练器：`drugbank_solver.py` / `davis_solver.py` / `kiba_solver.py`
  - loss：`utils.py: dirichlet_loss`，训练时使用 one-hot label（`torch.nn.functional.one_hot`）。
  - 指标：Accuracy、MCC、AUROC 等；并在 `evaluation()` 中输出 ROC/PR AUC，保存结果 CSV。
  - checkpoint：在验证集准确率提升时保存 `checkpoint.pth` 到当前 `runs/...`。

### 2.6 推理与结果文件

推理脚本：`/root/private_data/dd_model/EviDTI/infernce.py`

- 默认示例对 `case_graph_data`（`dataset/case_drugbank/`）进行推理，加载 `./runs/drugbank_model/checkpoint.pth`。
- 对每个样本输出：
  - `prob_list`：由 `alpha/sum(alpha)` 得到的正类概率
  - `p`：`[pred_label, true_label]`
  - `c/var`：不确定性指标（实现中为 `K/sum(alpha)`）
  - `ev`：`alpha-1`（evidence）
  - `bk_list`：中间量（代码写法为 `alphas - 1/sum(alphas)`，用于分析/可视化）
- 保存：`./runs/drugbank_model/result_test.csv`（脚本内写死路径，可按需调整）。

### 2.7 环境依赖与可复现注意事项

- 推荐环境文件：`/root/private_data/dd_model/EviDTI/environment.yaml`
  - Python 3.8、PyTorch 1.12 + CUDA 11.3、RDKit、torch-geometric 2.2、yacs、tensorboard 等。
- 数据/特征下载：readme 指向 Zenodo（特征文件与模型权重样例）
  - https://zenodo.org/records/14056305
- 代码与数据目录在不同脚本中存在“路径写死/平台相关”情况：
  - 部分特征提取脚本含 Windows 路径（如 `extract_p_emb.py`），需要改为本机 HuggingFace cache 路径或直接用 `from_pretrained(model_name)` 联网下载。
  - `extract_drug_2d_emb.py` 引用的 `dataset_dti.py`、`model.py` 在当前目录未提供；通常应直接使用 Zenodo 提供的预计算特征。
  - `extract_drug_3d_emb.py` 基于 PaddlePaddle/Pahelix（与 conda 环境不完全一致），更像是生成 3D 图特征的上游脚本；实际训练通常依赖已生成的 `*_d_3D_feature.npy`。

---

## 3. 两项目对比（共同点 / 差异点 / 复用建议）

### 3.1 共同点

- 都是**结构图 + 序列表示**的多模态学习范式：
  - ToxGIN：肽结构（residue graph）+ ESM2 残基 embedding + AAindex 理化特征。
  - EviDTI：药物结构（atom/bond/bond-angle graph）+ 蛋白序列 embedding（ProtT5）+ 药物 2D 特征。
- 都以图神经网络为核心，最终输出二分类/概率。

### 3.2 关键差异

- **研究对象/粒度**
  - ToxGIN：单条肽（residue-level 图），目标是毒性。
  - EviDTI：药物-蛋白对（drug graph + protein sequence），目标是相互作用。
- **图框架与图定义**
  - ToxGIN：DGL；图边由残基距离阈值构造。
  - EviDTI：PyTorch Geometric；药物图由化学键/键角等结构先验构造，并引入 BondAngleGraph 做高阶几何信息建模。
- **序列编码器**
  - ToxGIN：ESM2（`esm2_t36_3B_UR50D`）。
  - EviDTI：ProtT5（默认 1024 维 residue embedding）。
- **不确定性建模**
  - ToxGIN：标准 Sigmoid 概率输出（未显式建模 epistemic/aleatoric uncertainty）。
  - EviDTI：evidential learning 输出 Dirichlet 参数，可同时得到概率与“证据强度/不确定性”。

### 3.3 面向后续工作的复用建议（面向本仓库场景）

- 若本仓库后续关注“肽毒性/肽功能”预测：
  - 可直接复用 ToxGIN 的“结构→残基图→GIN”范式，并根据任务替换分类头/损失/指标。
  - 若希望给出可信度，可参考 EviDTI 的 evidential 输出方式，将 Sigmoid/Softmax 输出替换为 Dirichlet 参数并引入 `dirichlet_loss`。
- 若后续关注“肽-蛋白/肽-靶点相互作用”：
  - 可借鉴 EviDTI 的蛋白 LightAttention 分支（对 residue embedding 做 attention pooling），并将“药物图”替换为“肽结构图”或“肽-蛋白复合体图”（取决于数据可得性）。

---

## 4. 补充：可借鉴项目（面向“无需结构 + 多输出 + 可改造”的毒性框架）

更新时间：2026-01-15 14:08:37 +0800

你提出的方向是：**不要依赖 AlphaFold/ColabFold 结构**，但仍希望有“较强的序列表征（一般尺寸 PLM）”，并且输出要足够丰富（概率、把握、位点权重、证据、改造建议）。下面三个项目对“如何做多输出、如何做可解释、如何做可改造”有直接启发，可作为后续方案设计与工程实现的参考模板。

### 4.1 AMPainter：RL 驱动的“定向进化式”序列改造（可直接迁移到降毒设计）

项目路径：`/root/private_data/dd_model/AMPainter`

AMPainter 把“改造”拆成了清晰可复用的三段式流水线（非常适合迁移到毒性改造场景）：

1) **位点选择（Policy / Mutator）**：先决定“改哪里”  
   - `RLloop/network.py` 中的 `Mutator` 输出对每个位置的概率分布，按概率采样突变位置（本质是一个 learnable 的“位点重要性/可突变性”策略）。
2) **替换生成（fine-tuned PLM generator）**：再决定“换成什么”  
   - 通过对序列打 `<extra_id_*>` mask，再用 **Ankh** 的生成能力补全（`RLloop/func.py: embed_dataset()`），并且支持通过 `finetune/ankh_ft_noval.py` 在特定数据上微调生成器，让生成更贴近短肽分布。
3) **评分函数（Predictor / Reward）**：最后决定“好不好”  
   - `RLloop/func.py: scoring_function()` 用 HyperAMP 预测抗菌效力作为 reward，从而实现“闭环优化”。

可直接迁移到“毒性降低”任务的点：

- **把你的毒性模型当 reward**：reward 可以设为 `-(p_toxic)`，并引入“把握”项（例如对高不确定性候选加惩罚，避免模型瞎改）。
- **把改造过程变成“可解释日志”**：RL 天然能输出每一步改了哪些位点、采样概率、候选序列与得分，这些信息非常适合给后续 Agent 生成报告做依据。
- **HyperAMP 的序列表征也值得借鉴**：`HyperAMP/HyperAMP.py` 是超图神经网络（k-mer 超边 + TF‑IDF 权重），在“短肽 motif 结构化建模”上比普通 CNN 更像“可解释的组合规则”，可作为你毒性模型的一个轻量备选编码器/辅助分支。

### 4.2 PROTAC-STAN：结构信息注入的 PLM 表征 + 三体注意力（可借鉴其“特征提取与可解释融合”范式）

项目路径：`/root/private_data/dd_model/PROTAC-STAN`

你要“轻量但要 PLM”，PROTAC-STAN 的可复用点不在它的 PROTAC 场景本身，而在它对蛋白序列表征与注意力可解释的工程化组织方式：

- **可复用的蛋白表征提取流程**：`esm_embed/get_embed_s.py` 用 TorchDrug 封装了“从序列→残基视图→PLM→readout”的流水线，并将 embedding 以 map（`esm_s_map.pkl`）缓存起来；这非常适合你后续接外部证据库/历史数据时做“离线特征库”。
- **轻量适配器（adapter）**：`model.py: ProteinEncoder` 只是一个线性 adapter + FC，把高维 PLM embedding 映射到任务空间；这类 adapter 思路很适合“在不换大 backbone 的情况下，快速适配毒性任务 + 保持部署轻量”。
- **注意力可解释输出的工程接口**：`inference.py` 支持 `--save_att` 保存 attention maps，便于后续分析与可视化；毒性网站也可以做类似接口，把“位点权重/注意力热图/证据对齐结果”作为可下载的解释文件。
- **TAN（三体注意力）用于多模态融合的启发**：`tan.py` 构建三者交互张量并输出 attention map。对毒性任务可以把三路输入抽象成：
  - 序列残基 embedding（PLM）
  - 物化性质序列（charge/hydrophobicity 等逐位点特征）
  - motif/证据特征（高信息量 k-mer、数据库命中片段）  
  用类似 TAN 的方式融合，并将 attention map 作为“哪类特征在驱动毒性判断”的解释证据。

### 4.3 SCANS：小/中型 ESM2 + 多分支特征融合 + 信息论 motif（可直接借鉴到“毒性位点解释 + 特征融合”）

项目路径：`/root/private_data/dd_model/SCANS`

SCANS 的任务是碳酰化位点预测，但它的“特征表示与融合方式”对毒性预测很有参考价值：

- **用较小的 ESM2 做 residue embedding**：SCANS 使用 `esm2_t6_8M_UR50D`（8M）提取 320 维残基 embedding（见 `source_code/SCANS_K/loadData.py`），说明在“位点级/短窗口”任务上，小模型也能提供有效语义特征。结合你的约束（“不要超大模型，但需要一般尺寸 PLM”），可以考虑部署时用 ESM2 8M/35M 做在线推理，离线评估可再上更大模型做 teacher/对照（即便你不做蒸馏，也可用于模型选择）。
- **多分支融合结构**：`source_code/SCANS_K/model.py` 同时编码 ESM embedding、PSSM（20 维/位点）、PC（物化性质，10 维/位点），用 CNN/Transformer 等分别处理再融合；对毒性任务可替换为：
  - PLM embedding 分支（主干）
  - 物化性质分支（净电荷、疏水性、芳香性、Boman index 等逐位点特征）
  - motif/规则分支（见下一条）
- **信息论筛选的 motif 列表**：`selective_computation_based_motifs/*_carbonylation_related_motifs.txt` 给出带通配符的 motif（如 `KxKxLKS`），用于把“可解释片段证据”显式化。对毒性任务可以同样从你的训练集统计“高信息量毒性相关 motif”，用于：
  - 解释：报告中指出“哪些 motif 命中导致风险上升”
  - 改造：优先破坏高毒 motif、保留低毒 motif（与 AMPainter 的 evidence-guided edits 可以自然联动）

### 4.4 汇总：如何落到你的“毒性预测网站 + Agent 报告”

结合三项目的可复用点，建议将后续系统输出固定成可机读（JSON）+ 可视化（网页）的两套接口：

- **预测与把握**：evidential head（Dirichlet）输出 `p_toxic` + `uncertainty/total_evidence`（参考 EviDTI / 当前原型实现）。
- **位点权重**：融合“梯度归因（IG）+ 注意力权重（LA/TAN）”，并在网页展示 residue heatmap。
- **证据**：先用训练集相似序列（带 label）做证据卡片，后续接入外部数据库/文献后扩展为“可引用证据”。
- **改造建议（优先级）**：
  1) evidence-guided 最小编辑（来自相似 non-toxic 样本差异位点）
  2) attribution-guided 局部扫描（对高贡献位点做单点替换）
  3) RL/生成式优化（AMPainter 式闭环，reward 同时考虑“降毒 + 高把握 + 约束”）

---

## 5. 补充：更多毒性预测项目调研（ToxDL2 / ToxiPep / ToxMSRC / HyPepTox-Fuse）

更新时间：2026-01-15 15:46:27 +0800

本节补齐你补充的几个“毒性预测”项目。它们与本仓库目标（**无需结构 + 一般尺寸 PLM + 多输出：概率/把握/解释/证据/改造**）的关系可以概括为：

- **ToxDL2**：结构图 + PLM 的蛋白毒性预测范式（结构依赖强，但其 domain2vec / 特征缓存思路可借鉴）。
- **ToxiPep**：不依赖 AlphaFold 的“原子级（SMILES/RDKit）特征”分支（可作为序列模型的补充模态）。
- **ToxMSRC**：纯序列、较轻量的 Word2Vec(k-mer) + 多尺度 CNN + BiLSTM + residual（可作 baseline / 低资源备份方案）。
- **HyPepTox-Fuse**：多 PLM embedding + 传统描述符（CCD）融合与特征选择，并以 5-fold 集成输出概率集合（非常适合借鉴到“把握/不确定性 + 报告输出”）。

### 5.1 ToxDL 2.0（protein toxicity prediction：ESM2 residue embedding + 结构图 GCN + domain2vec）

项目路径：`/root/private_data/dd_model/ToxDL2`

**任务与输入输出**

- **任务**：蛋白毒性二分类/概率预测（protein-level）。
- **输入**：
  - 蛋白 3D 结构（PDB，README 建议用 AlphaFold2 预测）。
  - 蛋白域信息（InterPro domain 序列/列表，经 Word2Vec 得到 domain embedding）。
- **输出**：毒性概率（Sigmoid），并保存训练好的 `ToxDL2_model.pth`。

**核心方法（从代码提炼）**

- **节点/特征**：按 PDB 中 CA 原子提取残基序列，并用 `esm2_t33_650M_UR50D` 生成每个残基 1280 维 embedding（`src/dataset.py`）。
  - 支持对 `>1022` 的长序列分段提特征后拼接。
- **结构图建边**：残基两两距离 `<= 8Å` 建边（`pdb_to_graph(max_dist=8.0)`）。
- **图编码器**：4 层 `GCNConv` + `global_mean_pool` 得到 256 维图表示（`src/model.py: GCN`）。
- **域向量**：将 domain token 送入 skip-gram Word2Vec（仓库提供 `checkpoints/protein_domain_embeddings.model`），对一个蛋白的多个 domain embedding 做均值池化得到 `domain_vector`（`src/utils.py: get_domain_vector`）。
- **融合与分类**：`[protein_graph_emb(256) ; domain_vector(256/269)] → MLP → Sigmoid`（`src/model.py: ToxDL_GCN_Network`）。

**数据与文件组织（可复用的工程模式）**

- `data/pdb_data/{train,valid,test,independent}`：每条样本一个 PDB。
- `data/domain_data/{train,valid,test,independent}.domain`：每条样本以 block 形式存储 `>name / sequence / label / domain_vector`（向量维度 256 或 269）。
- `checkpoints/`：`ToxDL2_model.pth` + `protein_domain_embeddings.model*`（domain2vec 产物）。

**对你项目可借鉴点（面向“无需结构”）**

- **domain2vec 思路可迁移**：把“domain token”替换为“motif/k-mer token/结构规则 token”，用 Word2Vec 得到一个可解释的先验向量，再与 PLM 表征融合（对报告很友好）。
- **特征缓存**：先把 PLM residue embedding 离线算好并缓存（本仓库的证据检索/在线推理都需要类似思路）。
- **注意**：该项目强依赖 PDB/AlphaFold2，与“完全免结构”的主线不一致，更适合作为多模态增强或对照基线。

### 5.2 ToxiPep（peptide toxicity prediction：BiGRU+Transformer + 原子级图特征 + 交叉注意力）

项目路径：`/root/private_data/dd_model/ToxiPep`

**任务与输入输出**

- **任务**：肽毒性二分类/概率预测（peptide-level）。
- **输入**：肽序列（FASTA）；训练集以 `>pos*`/`>neg*` 区分标签（`Code/dataset.py: load_data_from_fasta`）。
- **输出**：2 类 softmax 概率；仓库提供 `best_model_0.9.pth`/`best_model_0.8.pth`。

**核心方法（从代码提炼）**

- **序列分支（上下文表征）**：
  - token embedding + sinusoidal positional encoding；
  - 2 层 BiGRU（双向，hidden 维度 `d_model//2`）；
  - 多层 TransformerBlock（MHA + FFN）；
  - 1D 池化（AdaptiveMaxPool1d）后 MLP 得到序列向量（`Code/model.py: peptide`）。
- **“原子级结构”分支（不依赖 3D 结构）**：
  - 预置每种氨基酸的 SMILES（`Code/atom_feature.py: aa_dict`），用 RDKit 得到每个残基内部原子邻接矩阵与 atom feature；
  - 对每个残基构造固定大小的 `(max_atoms=15, feat_dim=21)` 图特征，并在序列长度维度堆叠/裁剪到 `max_seq_len=50`，得到张量 `(50, 15, 21)`；
  - 将 `(B, 50, 15, 21)` 视作 2D “图像”输入，用多尺度 `Conv2d`（3×3/5×5/7×7/9×9）+ pooling 提取 1024 维结构向量（`Code/model.py: Structural`）。
- **融合（交叉注意力）**：把序列向量与结构向量线性投影到同一维度后做 CrossAttention，再拼接进分类器（`Code/model.py: ToxiPep_Model`）。
  - 实现上两路输入都是 “1 token”，CrossAttention 更像是双向 gated/投影融合；若要提升解释性，可把分支改成 per-residue 粒度再做真正的 token-level cross-attn。

**数据与切分**

- `Dataset/0.9/`：`train.txt` 7056（pos=3528/neg=3528），`test.txt` 1766（pos=883/neg=883）。
- `Dataset/0.8/`：`train.txt` 5544（pos=2772/neg=2772），`test.txt` 1388（pos=694/neg=694）。
- `Dataset/Independent set/independent test set.txt`：958（pos=479/neg=479）。

**对你项目可借鉴点**

- **“免 3D”但保留化学先验**：把 RDKit/SMILES 原子图特征作为可选分支，能在不做 AlphaFold 的情况下引入“侧链/原子”信息（适合作为 PLM 的补充特征）。
- **工程化注意**：训练脚本里数据路径写死为 `train.txt/test.txt`，实际数据在 `Dataset/` 子目录；复现时需调整路径或复制文件到 `Code/` 下。

### 5.3 ToxMSRC（peptide toxicity prediction：Word2Vec(k-mer) + 多尺度 CNN + BiLSTM + residual）

项目路径：`/root/private_data/dd_model/ToxMSRC`

**任务与输入输出**

- **任务**：肽毒性二分类/概率预测（peptide-level）。
- **输入**：FASTA（原始数据见 `Raw data/`）；模型输入特征为 Word2Vec 处理后的张量（示例见 `Example data/example_X.npz`）。
- **输出**：softmax 概率；训练产物 `model/ToxMSRC.h5`。

**核心方法（从代码提炼）**

- **序列编码**：将序列统一为长度 50（不足用 `X` padding，超长截断），提取 2-mer（stride=1）得到 49 个 token（`predictor.py: supple_X / Gen_Words`）。
- **Word2Vec 表征**：把 2-mer token 映射到向量，仓库提供 `word2vec_model/word2vec.model`（vector_size=96），最终得到输入 shape `(49, 96)`（`model.py`）。
- **网络结构**：三路多尺度 1D-CNN（kernel=3/5/7）→ Add(residual) → MaxPool → Conv1d → BiLSTM → Add(residual) → MLP → softmax（`model.py: ourmodel`）。
- **类别不平衡处理**：自实现 SMOTE（在扁平化 embedding 空间做插值）对正类过采样（`smote.py`）。

**数据与文件组织（可复用）**

- `Raw data/train_data.fasta`：6387（README 标注 pos=1818, neg=4569）。
- `Raw data/test1.fasta`：1126；`Raw data/test2.fasta`：582（两个独立测试集）。
- `Raw data/y_train.csv`、`y_test1.csv`、`y_test2.csv`：标签。
- `Example data/example_X.npz`：示例特征（含 `x_train/x_test`）。

**对你项目可借鉴点**

- **轻量 baseline**：在资源受限/需要快速上线时，Word2Vec(k-mer)+多尺度 CNN 是强 baseline，可作为 PLM 方案的对照或 fallback。
- **不平衡处理策略**：可对照 `SMOTE`/class-weight/focal loss 的影响，但要注意 SMOTE 生成的“插值样本”不一定对应真实肽分布，建议只作为实验对照。
- **工程注意**：`train.py` 默认读取 `word2vec_model/example_X.npz`，但仓库实际文件在 `Example data/example_X.npz`；`predictor.py` 的 OOV 零向量维度写成 150，与 Word2Vec 的 96 不一致，复现时需修正。

### 5.4 HyPepTox-Fuse（interpretable hybrid：多 PLM + CCD 融合 + attention/Transformer + 5-fold 概率集合）

项目路径：`/root/private_data/dd_model/toxin/HyPepTox-Fuse`

**任务与输入输出**

- **任务**：肽毒性二分类（peptide-level）。
- **输入特征**（以 Hybrid 版本为例）：
  - `f1`: ESM-2 residue embedding，shape `(1, L, 2560)`（默认配置 `input_dim_1=2560`）。
  - `f2`: ESM-1 residue embedding，shape `(1, L, 1280)`。
  - `f3`: ProtT5 residue embedding，shape `(1, L, 1024)`。
  - `fccd`: iFeatureOmega conventional descriptors（CCD），shape `(1, 887)`。
- **输出**：5 个 fold 的概率列表 `prob[fold]`，并以多数投票（≥3/5 超过阈值）给出最终 toxic/non-toxic（`predict.py: HyPepToxFuse_Predictor`）。

**核心方法（从代码提炼）**

- **多 PLM 表征交互**：三路特征先线性投影到 `gated_dim`，再做三组跨特征注意力 `attn_12/attn_23/attn_31`，拼接后送入 TransformerEncoder 融合（`src/architectures.py`）。
- **Hybrid 融合 CCD**：取 Transformer 的 `[CLS]` 输出，与 CCD 向量拼接后用多层 MLP 分类（MLP 逐层把维度减半，`num_mlp_layers=4`）。
- **CCD 特征选择落地方式**：`src/ccd_feature_order.py` 固化了多个 descriptor 的选取索引与全局排序（等价于“离线做过 feature ranking/selection 后，把 top-887 固化成规则”）；`src/feature_extractor.py: CCD` 按该规则拼装 887 维 CCD。
- **训练策略**：
  - focal loss（`src/focal_loss.py`）；
  - 额外加入监督式对比损失 `nt_bxent_loss`（`src/ntxent_loss.py`，权重 0.2）以增强 embedding 可分性；
  - 5-fold stratified k-fold，基于 MCC 选 best ckpt（`train_hybrid.py` / `train_nlp_only.py`）。

**数据与资源**

- README 声明使用 ToxinPred3 benchmark（doi: 10.1016/j.compbiomed.2024.108926），并提供 OneDrive 的特征/模型下载链接。
- 仓库自带 `raw_dataset/`：train/test 的 pos/neg FASTA（train: 4414/4414；test: 1104/1104）。
- 训练依赖预提特征目录 `Features/`（需外部下载或自行跑 `inferencer.py` 生成）。

**对你项目可借鉴点（很贴合“网站 + Agent 报告”）**

- **5-fold 概率集合 = 朴素不确定性**：不做 evidential 也能给出 `mean/var`、分位数、投票一致性等“把握”指标，产品化上很直观（可与 evidential 输出互补）。
- **“多特征融合 + 可解释”模板**：attention/Transformer 的注意力权重 + CCD 的特征权重/重要性（可用置换重要性或 SHAP 在离线做）都能直接服务报告。
- **工程启发**：提供 `Inferencer` 把“FASTA → 特征提取 → CSV 输出”串成可复用流水线；本仓库做 Web/Agent 报告时也建议固定成可追溯的离线特征与结果缓存。
- **注意**：其默认 ESM2(3B)/ProtT5-xl 属于“大模型”，与你的“轻量可部署”目标不完全一致；但其框架可用小/中型 PLM 替换实现同样的融合范式。

### 5.5 对本仓库的“可复用组件清单”（按优先级）

1) **HyPepTox-Fuse 的 5-fold 概率集合 + 置信度指标**：作为“把握”的 baseline（投票一致性/方差/分位数），与 evidential head 互相校验。
2) **ToxiPep 的 RDKit 原子级特征分支**：作为可选增强模态（免 3D 结构），尤其适合短肽/侧链驱动的毒性模式。
3) **ToxMSRC 的 Word2Vec(k-mer)+多尺度 CNN**：作为轻量对照模型与资源受限 fallback。
4) **ToxDL2 的 domain2vec/特征缓存**：迁移为 motif2vec / 规则 token2vec + 离线 embedding 索引，服务证据检索与解释。
