# 毒性预测项目计划 v2（Protein 版本：轻量 PLM + 多分支融合 + 可解释输出）

更新时间：2026-01-16 10:20:55 +0800

本文档用于：在 v1（可跑通、可复现、有 time OOD split）的基础上，按“轻量可部署但必须使用一般尺寸 PLM”的约束，吸收 `PROTAC-STAN` / `SCANS` 的可复用工程范式，形成 v2 版本的**特征表示、融合结构、解释接口与评估协议**，并记录 v2 的实现与对照结果。

---

## 0. v1 当前进展（已完成，可复现）

### 0.1 数据与切分（time split）

- 数据：ToxDL2 protein（time split v1）
- 切分目录：`data/toxicity_data_v1/splits/protein_toxdl2_time_v1/`
- 规模：
  - train：14700（pos=4900）
  - val：931（pos=78）
  - test（pre-year）：1846（pos=112）
  - independent（post-year）：4862（pos=152）

### 0.2 v1 对照组（预测 + 把握）

共同点：全部使用 Dirichlet evidential 输出（`p_toxic` + `uncertainty/total_evidence`），阈值在 val 上按 MCC 扫描选取后固定评估 test/independent。

1) **Baseline‑A（CNN evidential，无 PLM）**  
产物：`artifacts/protein_plan_test_v1_toxdl2/summary.json`  
关键指标（val-best 阈值）：
- test：AUPRC=0.6700，BACC=0.7943
- independent：AUPRC=0.3472，BACC=0.7166

2) **PLM‑mean（ESM2‑t12‑35M frozen + pooled embedding）**  
产物：`artifacts/protein_plan_test_v1_toxdl2_plm/summary.json`  
关键指标（val-best 阈值）：
- test：AUPRC=0.8186，BACC=0.8679
- independent：AUPRC=0.4200，BACC=0.8429

3) **PLM‑contact‑graph（ESM2 last‑layer attention → pseudo‑contact 图 + 轻量 GNN）**  
产物：`artifacts/protein_plan_test_v1_toxdl2_plm_contact/summary.json`  
关键指标（val-best 阈值）：
- test：AUPRC=0.8295，BACC=0.9255
- independent：AUPRC=0.4779，BACC=0.8640

结论（v1）：在不依赖外部结构库/不做结构预测的前提下，小型 PLM（35M）+ pseudo‑contact 图分支可稳定提升 time OOD（independent）。

---

## 1. v2 总目标与设计约束

### 1.1 目标（面向“网站 + Agent 报告”）

- 预测输出：`p(toxic)` + `uncertainty/total_evidence`（把握）+ 可用于阈值决策的 calibration 线索
- 解释输出（可机读）：位点权重/热点区域、关键 motif 命中、最相似证据样本、与预测一致性（证据强度 vs 不确定性）
- 改造建议（先规则/启发式，后 RL）：优先在“高风险位点/高毒 motif”上做最小编辑，提供候选及其风险/把握变化

### 1.2 约束（轻量可部署）

- PLM 允许：ESM2 8M/35M/150M 级别；默认用本地 `esm2_t12_35M_UR50D`
- 不允许：依赖 AlphaFold/外部结构库作为前置步骤（但可利用 PLM 内部信号形成 pseudo‑structure）
- 训练侧允许离线预计算/缓存特征；推理侧支持 fast 模式与 full 模式

---

## 2. v2 方案：多分支特征表示 + 可解释融合（借鉴 PROTAC‑STAN / SCANS）

### 2.1 为什么 v2 不能只停留在“PLM mean / contact 图”

v1 的 PLM 分支已经带来强泛化，但仍缺少两类能力：

- **可解释的特征融合接口**：需要把“PLM 语义”“物化性质”“motif/证据”统一到一个融合模块，并输出可解释的 attention/map（参考 PROTAC‑STAN 的 TAN 思路）
- **显式可追溯的 motif/规则**：需要像 SCANS 一样把“高信息量 motif”显式化，既用于解释也用于改造优先级

### 2.2 三路特征（v2 主线）

对应 `docs/项目概览.md` 的：
- PROTAC‑STAN：离线 embedding 缓存 + adapter + attention map 导出 + TAN 融合
- SCANS：多分支（ESM/物化性质/motif）+ motif 列表显式化

v2 将三路输入抽象为：

1) **PLM 语义向量（主干）**
   - 表征：ESM2 pooled embedding（mean over residues；长序列滑窗聚合）
   - 方式：冻结 PLM + 轻量 adapter（与 PROTAC‑STAN 的 ProteinEncoder 类似）

2) **物化性质向量（辅助分支）**
   - 表征：序列组成（AA composition）+ 基础理化统计（charge/hydrophobicity 等）
   - 目标：补齐 PLM 对“可解释理化规律”的显式建模，并提供可对齐的解释字段

3) **motif/证据向量（可解释分支）**
   - 表征：从训练集挖掘的高信息量 k-mer（3–5-mer）/motif 的计数或频率向量（显式列表）
   - 目标：解释（命中哪些 motif 推高风险）+ 改造（优先破坏高毒 motif）

### 2.3 融合模块（v2 的“创新点落地”）

实现两套可切换融合（都保持轻量）：

- **TAN（三体注意力，参考 PROTAC‑STAN/tan.py）**：三路向量经 adapter 后进入 TAN，输出 fused embedding，并可导出 attention map（用于“哪类特征驱动预测”的解释）
- **trilinear gating（更轻量的对照）**：元素级三路交互 `a*b*c` + MLP（作为 TAN 的消融基线）

输出保持 evidential：fused embedding → Dirichlet head → `alphas` → `p_toxic` + `uncertainty/total_evidence`。

### 2.4 位点权重/解释（v2 必须补齐）

v2 将解释拆成“可实现且可追溯”的两层：

1) **序列级解释（融合层）**：TAN attention map（模态/特征交互强度）
2) **位点级解释（pseudo‑structure 层）**：使用 v1 已实现的 `PLM‑contact pseudo‑structure`，从 last‑layer attention 得到：
   - pseudo‑contact 图中心性/度（高连接位点）
   - 可选：CLS→residue attention（若实现中可稳定提取）
   - motif 命中位置 overlay（把“结构信号”与“motif 规则”对齐到位点）

说明：位点解释不要求参与训练（可在推理时计算/导出），以避免训练期的巨额开销。

---

## 3. v2 工程化（必须做：离线特征库/缓存）

### 3.1 为什么必须缓存（对齐 PROTAC‑STAN 的 esm_s_map.pkl 思路）

ToxDL2 protein train=14700，直接在每个 epoch 反复跑 PLM 会极大拖慢迭代。v2 默认把以下内容离线缓存：

- ESM2 pooled embedding（float16/float32）
- 物化性质全局特征
- 选定 motif 的计数/频率向量
- （可选）融合 embedding，用于 embedding 检索索引（cosine）

### 3.2 v2 产物结构（建议）

缓存：
- `data/feature_cache_v2/protein_toxdl2_v2/*`

训练/评估产物：
- `artifacts/protein_plan_test_v2_toxdl2/*`

---

## 4. v2 训练与评估协议（对照一致）

- 数据：同 v1 time split（不改动）
- 不平衡：class_weights + focal(gamma=2.0) + KL warmup（延续 v1）
- 阈值：val 扫描 MCC（或 BACC）选阈值，固定评估 test/independent
- 指标：AUROC / AUPRC(PR) / BACC / MCC / F1 / Precision / Recall
- 对照组（至少三组）：
  - v1 PLM‑mean（8.10）
  - v2 trilinear gating（消融）
  - v2 TAN（主模型）
  - （可选）v2 TAN + contact‑graph 融合（v2.1）

---

## 5. v2 本轮要做的实现与测试（先规划后执行）

### 5.1 实现清单（v2.0）

1) 特征与缓存
   - 训练集上挖掘 motif 列表（3–5-mer，top‑N，可记录 pos/neg log-odds）
   - 生成并保存：PLM pooled embedding / physchem features / motif features（train/val/test/independent）
2) 模型
   - 三路 adapter（PLM/phys/motif）
   - 两套融合：trilinear gating + TAN
   - evidential head（保持与 v1 一致）
3) 评估与产物
   - 输出 `summary.json` + `test_predictions.csv` + `independent_predictions.csv`
   - 保存（可选）TAN attention map（抽样/少量样本），供可视化与报告

### 5.2 复现命令（预计新增脚本）

```bash
# 1) 构建 v2 特征缓存 + motif 列表
python scripts/build_protein_toxdl2_feature_cache_v2.py --plm-path /root/group_data/qiuleyu/esm2_t12_35M_UR50D

# 2) 训练并评估 v2（TAN / trilinear）
python scripts/test_protein_plan_v2_toxdl2.py --cache-dir data/feature_cache_v2/protein_toxdl2_v2 --fusion tan
python scripts/test_protein_plan_v2_toxdl2.py --cache-dir data/feature_cache_v2/protein_toxdl2_v2 --fusion trilinear
```

### 5.3 执行结果

#### 5.3.1 v2 特征缓存（已执行）

更新时间：2026-01-16 10:33:02 +0800

缓存目录：

- `data/feature_cache_v2/protein_toxdl2_v2/meta.json`
- `data/feature_cache_v2/protein_toxdl2_v2/motifs.tsv`（3–5-mer，top‑512，min_df=5，log‑odds 挖掘）
- `data/feature_cache_v2/protein_toxdl2_v2/plm_{split}.npy`（ESM2 pooled embedding，float16）
- `data/feature_cache_v2/protein_toxdl2_v2/phys_{split}.npy`（physchem 全局特征，33 维）
- `data/feature_cache_v2/protein_toxdl2_v2/motif_{split}.npy`（motif 频率向量，512 维）

#### 5.3.2 v2 对照：trilinear gating（已执行）

更新时间：2026-01-16 10:43:35 +0800

产物目录：

- `artifacts/protein_plan_test_v2_toxdl2_trilinear/summary.json`
- `artifacts/protein_plan_test_v2_toxdl2_trilinear/test_predictions.csv`
- `artifacts/protein_plan_test_v2_toxdl2_trilinear/independent_predictions.csv`
- `artifacts/protein_plan_test_v2_toxdl2_trilinear/retrieval_fused.joblib`

设置摘要：

- 输入：cached `plm(480)` + `phys(33)` + `motif(512)`
- fusion：`trilinear`；adapter_dim=64；batch_size=256

结果摘要（val-best 阈值）：

- test：AUROC=0.9753，AUPRC=0.7493，BACC=0.8648，MCC=0.7588
- independent：AUROC=0.9466，AUPRC=0.4537，BACC=0.7967，MCC=0.5057

#### 5.3.3 v2 主模型：TAN（三体注意力）（已执行）

更新时间：2026-01-16 10:51:21 +0800

产物目录：

- `artifacts/protein_plan_test_v2_toxdl2_tan/summary.json`
- `artifacts/protein_plan_test_v2_toxdl2_tan/test_predictions.csv`
- `artifacts/protein_plan_test_v2_toxdl2_tan/independent_predictions.csv`
- `artifacts/protein_plan_test_v2_toxdl2_tan/retrieval_fused.joblib`
- `artifacts/protein_plan_test_v2_toxdl2_tan/tan_att_samples.npz`（抽样保存 attention map，用于可视化/报告）

设置摘要：

- 输入：cached `plm(480)` + `phys(33)` + `motif(512)`
- fusion：`tan`；adapter_dim=32；tan_heads=2；batch_size=64
- 备注：TAN 的显存/计算随 `adapter_dim^4 * batch_size` 增长；若 `adapter_dim` 太大或 batch 太大易 OOM

结果摘要（val-best 阈值）：

- test：AUROC=0.9826，AUPRC=0.7561，BACC=0.8628，MCC=0.7316
- independent：AUROC=0.9482，AUPRC=0.4643，BACC=0.8078，MCC=0.4996

#### 5.3.4 v1/v2 小结（当前结论）

- v2 的价值（当前已落地）：**离线特征库/缓存** + **显式 motif 特征** + **可导出的融合 attention（TAN）**，为后续“证据+改造”Agent 提供可追溯中间产物。
- 指标对比（重点看 time OOD/independent）：
  - 相对 v1 PLM‑mean（8.10）：v2 在 independent 的 AUPRC 提升到约 `0.4623–0.4643`；BACC 也可提升到 `0.8705`（见 5.3.5）。
  - 相对 v1 PLM‑contact‑graph（8.11）：v2 可在 independent 上达到相近甚至更高的 BACC（`0.8705 > 0.8640`），但在 ranking（AUPRC）与相关阈值指标（MCC/F1）上仍落后（见 5.3.5 与 8.11.6 对照）。
- 下一步（v2.1 优先级）：把 v1 的 `contact-graph` 向量作为额外分支接入 v2 融合（或改用更省显存的“3-token cross-attn 融合”替代 TAN 训练），在不牺牲 v1 指标的前提下补齐解释与工程化能力。

#### 5.3.5 复跑对照：统一超参（adapter_dim=32，batch_size=64）

更新时间：2026-01-16 11:07:43 +0800

目的：对齐“轻量可部署”的超参与显存约束，在相同 `adapter_dim=32` / `batch_size=64` 下比较 trilinear 与 TAN 的差异。

1) trilinear（ad32/bs64）  
产物：`artifacts/protein_plan_test_v2_toxdl2_trilinear_ad32_bs64/summary.json`

- test：AUPRC=0.7461，BACC=0.9219，MCC=0.7329
- independent：AUPRC=0.4623，BACC=0.8705，MCC=0.4805（recall=0.7961，precision=0.3184）

2) TAN（ad32/bs64）  
产物：`artifacts/protein_plan_test_v2_toxdl2_tan_ad32_bs64/summary.json`

- test：AUPRC=0.7561，BACC=0.8628，MCC=0.7316
- independent：AUPRC=0.4643，BACC=0.8078，MCC=0.4996（recall=0.6447，precision=0.4170）

---

## 6. 扩展规划：轻量级序列信息挖掘、证据搜集与（可选）几何增强

更新时间：2026-01-16 11:31:01 +0800

本节回答两个问题：

1) 如果引入更多“轻量级工具”去搜集与挖掘序列相关信息（不强依赖结构预测），还有哪些可做、能带来哪些增益？
2) 若你允许少量离线结构（例如 xfold / 其他快速折叠），GeoGNN 这类更“蛋白几何友好”的网络是否值得上？如何不破坏“轻量可部署”的主线？

### 6.1 优先级 A（强烈建议先做）：同源检索 → 证据库 → MSA/profile 特征

这条路线的优势：**完全不需要 3D 结构预测**，但能同时增强泛化、解释与“证据可追溯性”。也最符合你想做的网站/Agent 报告。

#### 6.1.1 同源检索与证据落地（可机读）

建议工具（按常用与可扩展排序）：

- `mmseqs2`：速度快、可大规模；适合构建你自己的“证据库/相似序列库”。
- `jackhmmer` / `hmmersearch`：擅长远同源；适合构建 MSA/profile。
- `blastp` / `diamond`：作为对照或快速 baseline。

证据库建议来源（优先可离线镜像/可下载的）：

- UniProtKB（Swiss‑Prot/TrEMBL）：用于“功能注释 + 实验/推断证据等级”。
- Tox‑Prot（UniProt 的 toxin subset）：用于高置信“毒素/毒蛋白”正例证据与注释字段。
- （可选）毒素/毒力相关数据库（如 VFDB 等）：用于扩展覆盖，但要更严格控制标签噪声。

建议输出一套统一证据 schema（用于网页/Agent）：

- query 信息：序列、长度、预测 `p_toxic/uncertainty`、所用模型版本
- hit 列表：hit_id、identity、coverage、evalue、物种、毒素相关关键词/注释、数据库来源、证据等级
- 证据汇总：top‑k 命中分布、是否命中 Tox‑Prot、命中一致性（预测与 hit 标签是否一致）

#### 6.1.2 MSA/profile 特征（对齐 SCANS 思路）

从同源检索结果构建 MSA 后，可以抽取两类信息：

- **逐位点 profile**：PSSM（20 维）、保守性/熵、gap 率、MSA depth 等（解释性非常好）
- **序列级 summary**：平均保守性、低复杂区占比、MSA 深度分位数等（轻量、好部署）

把 profile 特征接入模型的方式（两档）：

- v2.2（轻量）：只用序列级 summary（直接 concat 到 v2 三路特征）
- v2.3（更强）：逐位点 profile 作为第四路“per‑residue 分支”，与 PLM residue embedding 做 cross‑attn 或轻 CNN 融合，再输出位点解释（类似 SCANS 的多分支思想）

### 6.2 优先级 B（不依赖结构预测）：从 PLM 内部再挖一层“pseudo‑structure / pseudo‑annotation”

你已经有 v1 的 `PLM-contact pseudo-structure`，这里还有几个可继续提升的方向：

- **边特征更丰富**：把 `attention score`、`APC 后分位数`、`|i-j|`（序列距离）、是否同一窗口（长序列 chunk）等作为 edge features，而不是只有 edge_index。
- **图编码器更贴任务**：从 mean aggregation 升级到 edge-aware 的 message passing（仍可保持 2–3 层、hidden=128/256），以提高对“关键长程相互作用”的建模。
- **位点解释更稳**：把“pseudo-contact 图中心性/度”+“motif 命中位置”+“PLM residue saliency（梯度/遮挡）”合并成一套位点热图输出，提升可解释性一致性。

### 6.3 优先级 C（可选，离线增强）：xfold/快速折叠 → 几何图 → GeoGNN/几何模型（作为 teacher 或增强分支）

如果你愿意引入少量结构（强调：建议先离线、再迁移到轻量主线），优化空间依然很大：

#### 6.3.1 为什么 GeoGNN 有用、但不建议一上来就“全量在线依赖结构”

- GeoGNN/几何 GNN 的优势来自可靠几何输入（距离/角度/局部坐标系）。
- 但“在线每条序列都折叠”会把系统变成结构预测产品，违背你要的便捷性；同时结构误差会把噪声注入训练。

因此更推荐的定位：

- **离线 teacher / 对照**：在你有结构的子集上训练/评估几何模型，得到更强的判别或更可靠的位点解释；
- **知识迁移到轻量分支**：把 teacher 的信号转成可学习目标（例如：pseudo-contact 边权回归、位点热图对齐、ranking consistency），从而提升不依赖结构的主模型。

#### 6.3.2 “结构增强但仍轻量”的落地方式（两档）

- v2.4（保守）：只对一部分样本跑 xfold，抽取简单结构统计（接触密度、二级结构比例、局部几何分布）作为额外 feature；线上不需要结构。
- v2.5（更强）：对结构子集构建 residue graph（距离阈值边 + 角度/距离边特征），训练 GeoGNN；把输出的位点重要性/图表示用于蒸馏到 `PLM-contact` 分支。

### 6.4 与当前 v2 的直接衔接（建议的实施顺序）

按“收益/成本/不破坏轻量主线”的优先级排序：

1) **先做证据检索与 schema 固化**：把 mmseqs2/hmmer 的 hit 结果写成统一 JSON，并接入网页与 Agent 报告（收益最大）。
2) **再做 MSA summary 特征**：先把 MSA depth/entropy 这类轻量 summary 接到 v2（无需改太多网络）。
3) **升级 pseudo-contact 图分支**：补 edge features + 更强但仍轻量的图编码器，目标是至少追平/超过 v1 contact 的 AUPRC。
4) **最后再引入 xfold/GeoGNN（离线）**：用于 teacher/迁移与“解释可信度”提升，而不是让线上强依赖结构预测。

### 6.5 （网络检索）近期更高效的结构预测方法清单（用于 GeoGNN 的离线结构生成）

更新时间：2026-01-16 11:31:01 +0800

说明：以下条目以“更高效/更易扩展/更适合大规模离线结构生成”为主；是否能直接落地取决于**代码/权重是否公开**、硬件需求与许可证。这里给出**标准来源链接**（arXiv/官方技术报告），便于你逐一评估与尝试。

#### 6.5.1 以“效率/可扩展推理”为核心的新模型/变体（重点关注）

1) **Protenix‑Mini（few‑step diffusion + compact architecture）**  
来源：arXiv:2507.11839《Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM》  
https://arxiv.org/abs/2507.11839  
要点（从摘要）：用 few‑step ODE sampler 替代多步采样、对 Pairformer/Diffusion blocks 做裁剪与轻量化、引入“可切换 pLM”以替代传统模块，目标是降低推理开销。

2) **Protenix‑Mini+（进一步提升可扩展性）**  
来源：arXiv:2510.12842《Protenix-Mini+: efficient structure prediction model with scalable pairformer》  
https://arxiv.org/abs/2510.12842  
要点（从摘要）：压缩非可扩展操作以缓解 cubic 复杂度、减少冗余模块、few‑step sampler 加速 atom diffusion；并报告在可接受精度代价下实现“90%+ 计算效率提升”（摘要表述）。

3) **SeedFold（线性三角注意力，强调复杂度降低以便 scaling）**  
来源：arXiv:2512.24354《SeedFold: Scaling Biomolecular Structure Prediction》  
https://arxiv.org/abs/2512.24354  
要点（从摘要）：提出 linear triangular attention 以降低计算复杂度并支持更高效的 scaling（并宣称在 FoldBench 上优于 AlphaFold3 的若干任务）。

4) **Neural ODE Evoformer（连续深度：内存/时间可调）**  
来源：arXiv:2510.16253《Protein Folding with Neural Ordinary Differential Equations》  
https://arxiv.org/abs/2510.16253  
要点（从摘要）：把 Evoformer 从离散堆叠改为连续深度 ODE 形式，使用 adjoint method 达到“深度维度常数内存”，可用自适应求解器做运行时与精度的折衷。

#### 6.5.2 AF3 路线的“可替代实现/工程化复刻”（复杂体系更强，但落地要看开放程度）

1) **HelixFold3（AF3 能力复刻方向的技术报告）**  
来源：arXiv:2408.16975《Technical Report of HelixFold3 for Biomolecular Structure Prediction》  
https://arxiv.org/abs/2408.16975  
要点（从摘要）：面向“蛋白+核酸+常见配体”等更一般的生物分子结构预测，目标是复刻/对齐 AlphaFold3 能力；如果你后续要做“蛋白‑小分子/多链复合物”的 GeoGNN，这条路线更相关。

2) **MegaFold（系统级优化：更偏训练加速，但可关注其 kernel/caching 思路）**  
来源：arXiv:2506.20686《MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models》  
https://arxiv.org/abs/2506.20686  
要点（从摘要）：围绕 AF3 训练的系统瓶颈提出缓存、Triton kernel、算子融合等优化；对你来说更像“工程加速参考”，不一定直接给你一个新的可推理模型。

#### 6.5.3 这些方法如何用于 GeoGNN（落地建议）

- **离线结构生成**：用上述任一结构预测器输出 PDB/mmCIF → 解析成 residue graph（Cα 节点、8Å/10Å 距离阈值建边）→ 计算边特征（距离/方向/序列距离）→ GeoGNN 训练。
- **避免全量折叠成本**：先做序列去重/聚类（例如 90% identity），只对 cluster representative 折叠，再把结构特征映射/复用到同簇样本（可显著降计算）。
- **保持轻量主线**：建议把 GeoGNN 作为“离线 teacher/增强分支”，把收益迁移回本仓库的结构无关分支（PLM‑mean / PLM‑contact pseudo‑structure），确保网站推理不被结构预测卡住。

### 6.6 结构预测模型可行性评估（算力 / 硬盘 / 时间；用于 GeoGNN 离线结构）

更新时间：2026-01-16 14:43:30 +0800

本节目标：把“近期更高效结构预测方法”落到工程可行性层面——**你现在/近期能不能用、要花多少硬件与存储、结构生成的吞吐大概是什么量级**。  
重要说明：不同项目的计时口径差异很大（是否包含 MSA 搜索/模板检索/多 seed 多 sample/后处理），因此这里不强行横向排名；缺乏公开量化数据的条目会明确标注“需实测/未公开”。

#### 6.6.1 结论（按你当前偏好：轻量可部署 + 可离线结构增强）

- **最推荐（立刻可做、离线成本可控）**：`Protenix-mini/tiny`（few-step diffusion，可按需开关 MSA）或 `ESMFold`（单序列、无数据库）用于“只折叠聚类代表序列”的离线结构库。
- **可做但更重**：`ColabFold(本地 MSA)` 与 `HelixFold3(reduced_dbs)` 都需要**数百 GB 级数据库**与较强 CPU/SSD；HelixFold3 官方还建议单卡 ≥32GB 显存，与你当前“轻量部署”目标冲突更大。
- **作为启发/工程参考**：`SeedFold`（线性三角注意力）目前更像“架构/复杂度路线”，是否开源/是否有可直接跑的权重不明确；`MegaFold` 是 AF3 训练侧系统优化，不是直接可用的折叠器。
- **加速 AF2-style pipeline 的一个分支**：`openFoldODE`（Neural ODE Evoformer）给出 8GB GPU 上的推理时间基准，但它需要 OpenFold/AF2 的输入特征生成（本质仍要走数据/特征管线），更适合你将来做“结构侧 teacher”而非线上折叠服务。

#### 6.6.2 资源/时间对照表（含标准来源）

> 你已下载的 arXiv PDF 位于：`docs/papers/structure_prediction/`

| 方法 | 是否开源/可得 | 是否依赖 MSA/数据库 | 主要硬盘开销（来源） | 显存与长度上限（来源） | 推理时间（来源） | 适合怎么用在 GeoGNN |
|---|---|---|---|---|---|---|
| **Protenix（AF3 复刻）** | ✅ 代码开源（Apache 2.0）：https://github.com/bytedance/Protenix | 默认 ✅ MSA（可走 colabfold compatible MSA）；也支持 `mini_esm` 关闭 MSA（但需额外 PLM） | 权重：base `~1.4GB`；mini `~512MB`；tiny `~423MB`；mini_esm `~517MB`（下载 URL/HTTP header：`af3-dev.tos-cn-beijing.volces.com`，见 `protenix/web_service/dependency_url.py`）；数据 cache：CCD `~408MB` + RDKit `~120MB` + clusters `~21MB`（同 URL） | 推理显存示例（BF16 mixed，部分 FP32）：`Ntoken=1000,Natom=10000 → 18.2GB`（https://raw.githubusercontent.com/bytedance/Protenix/main/docs/model_train_inference_cost.md）；并给出 A100-80G 上长度/时间曲线（https://raw.githubusercontent.com/bytedance/Protenix/main/assets/inference_time_vs_ntoken.png） | 同上 cost doc：`Ntoken=500 → 17s`，`1000 → 59s`，`2000 → 226s`（均为参考配置；含 N_cycle=10,N_step=200 等设置） | **离线折叠器**：对 toxin/protein 聚类代表做结构生成；复杂体系/多链/配体更占优，但不建议线上强依赖 |
| **Protenix‑Mini / Mini+（few-step diffusion）** | 论文：arXiv:2507.11839 / 2510.12842；工程上已在 Protenix repo 提供 mini/tiny 配置（`configs/configs_model_type.py`） | 可 ✅ MSA，也可 `mini_esm` ❌ MSA（但需 ESM2‑3B） | mini/tiny 权重如上；若用 `mini_esm` 还需 ESM2‑3B 权重 `~5.3GB`（同 `dependency_url.py`/HTTP header） | 未看到“统一口径的推理显存表”；但 mini/tiny 默认 `N_cycle=4, N_step=5`（见 `configs/configs_model_type.py`），通常会比 base 更省（需实测） | 未公开统一基准（需实测）；Mini+ 摘要提到“90%+ 计算效率提升”但未给出可直接复现的秒数表（以 paper 为准） | **优先候选**：在不追求极致精度时，用 few-step 生成大量离线结构，作为 GeoGNN teacher/对照 |
| **HelixFold3（AF3 能力复刻，Paddle）** | ✅ 代码与权重下载指引：https://raw.githubusercontent.com/PaddlePaddle/PaddleHelix/dev/apps/protein_folding/helixfold3/README.md | ✅ 需要 MSA 工具与遗传数据库（`reduced_dbs/full_dbs`） | `reduced_dbs`：下载约 `190GB`，解压约 `530GB`（README “Usage”）；权重 zip `~1.3GB`（https://paddlehelix.bd.bcebos.com/HelixFold3/params/HelixFold3-params-20250714.zip） | README “Resource Usage”：建议单卡推理 ≥`32GB` 显存；A100-40G bf16 约支持 `~1200 tokens`；V100-32G fp32 约 `~1000 tokens` | README 未给出“秒/蛋白”基准（需实测）；且 pipeline 还包含 MSA/模板等步骤 | **更偏重型离线**：若你将来要做“蛋白‑核酸/蛋白‑小分子/多链复合物”的几何 teacher，这条路线更相关，但成本更高 |
| **ColabFold（AF2 + MMseqs2，本地 MSA）** | ✅ 代码：https://github.com/sokrypton/ColabFold | ✅ 强依赖数据库（本地 MSA/模板） | 本地 DB（`setup_databases.sh`）：`uniref30_2302.db.tar.gz ~99GB`，`colabfold_envdb_202108.db.tar.gz ~120GB`，模板 `pdb100_foldseek_230517.tar.gz ~18GB`（脚本与 HTTP header）；wiki 还指出“预计算 index 全驻 RAM 的服务器推荐 2TB RAM”（https://raw.githubusercontent.com/wiki/sokrypton/ColabFold/Home.md） | 显存/时间与 AF2 实现/长度/recyles 强相关；这里不做估计（官方文档未给统一表） | 同上：MSA 搜索往往是总时延主要来源，且取决于是否预建 index/CPU 线程数/SSD | **更稳的离线基线**：当你未来做外部数据扩展，需要最可靠的高精度结构，可在少量代表序列上跑作为 teacher/标定 |
| **ESMFold（MSA-free baseline）** | ✅ 代码/文档：https://raw.githubusercontent.com/facebookresearch/esm/main/README.md | ❌ 不需要 MSA/数据库 | 权重 `~2.6GB`：https://dl.fbaipublicfiles.com/fair-esm/models/esmfold_3B_v1.pt | 资源口径依赖长度；ESM 官方提供 `--chunk-size`（降显存、降速）与 `--cpu-offload`（用 RAM 换显存）参数（见 README “esm-fold” CLI） | 官方 README 未给出统一“秒/蛋白”基准（需实测/参考论文）；但作为单序列折叠器非常适合批量离线生成 | **轻量离线结构库首选之一**：无 DB 成本，适合对聚类代表序列快速折叠，为 GeoGNN 生成 teacher 结构 |
| **openFoldODE（Neural ODE Evoformer）** | ✅ 论文：arXiv:2510.16253（PDF 已下载）；✅ 代码：https://github.com/ariellesanford/openFoldODE | ❌ 非端到端单序列折叠器：依赖 OpenFold/AF2 生成的 Evoformer inputs（意味着仍要走数据/特征生成） | README：下载 PDB70 数据需要 `113GB` 存储（https://raw.githubusercontent.com/ariellesanford/openFoldODE/main/README.md）；另需 AF 参数（README 中给脚本） | 论文给出测试环境：Quadro P4000 `8GB`（arXiv PDF 第 6 页） | 论文报告：平均 `4.85 s/protein`（≈`0.0300 s/residue`），对比 OpenFold Evoformer `65.06 s/protein`（arXiv PDF 第 6 页） | **加速结构侧 teacher 的“组件级方案”**：当你确实需要 AF2-style 特征/结构时，可用来压缩 Evoformer 的推理时间/显存 |
| **SeedFold（线性三角注意力）** | 论文：arXiv:2512.24354（PDF 已下载）；开源/权重：未在 paper 中给出可直接复现的下载指引（需进一步确认） | 未明确 | paper 提供的是“复杂度/模块基准”，不是开箱即用的落地资源清单 | paper Fig.3(b) 给出 attention 模块的 peak memory/time cost 曲线（单位 MB/ms），但**不是端到端折叠推理资源表** | 未给出“秒/蛋白”端到端推理基准 | **方法论启发**：若你后续要做自研结构分支/加速 Pairformer，可参考其线性三角注意力设计 |
| **MegaFold（系统级优化）** | 论文：arXiv:2506.20686（PDF 已下载） | N/A | 关注训练/系统优化（kernel/caching/activation memory），不是一个独立折叠器 | N/A | N/A | **工程参考**：当你未来需要对重型结构模型做系统优化时可借鉴 |

#### 6.6.3 对你当前“GeoGNN + 毒性任务”的最小可行落地（建议顺序）

1) **先用不依赖结构的主线继续迭代 v2（你现在已经做到）**：线上只输出 `p_toxic + uncertainty + 位点解释 + 证据检索`。  
2) **离线结构库只对 90% 聚类代表做折叠**：优先尝试 `Protenix-mini/tiny`（few-step）或 `ESMFold`（MSA-free）。  
3) **GeoGNN 先做 teacher/对照，不做线上硬依赖**：用结构 teacher 输出位点热图/结构表征，再蒸馏回 `PLM-contact pseudo-structure` 分支，避免结构预测成为产品瓶颈。
