# 毒性预测项目计划 v3（证据检索 + MSA/Profile + pseudo‑structure；先跑通后逐步替换为真实同源检索）

更新时间：2026-01-16 20:33:49 +0800

目标：把你在 v2 里提出的“**证据可追溯**（可机读 hit 列表）+ **MSA/profile 特征**（提升泛化/解释）”先落到一个**可运行的最小框架**里；当前环境里尚未部署 `mmseqs2/hmmer/blast/diamond` 与外部大库，所以先用 **internal TF‑IDF evidence** 与 **可选 A3M 输入**把管线跑通，之后你部署好工具与库后再把 backend 切换到真实同源检索。

---

## 1. 本轮交付（已实现且可跑通）

### 1.1 证据检索：统一 schema + internal TF‑IDF evidence（可机读）

已新增模块（用于网页/Agent 报告的结构化中间产物）：

- `toxapp/evidence/schema.py`：`EvidenceQuery/EvidenceHit/EvidenceSummary/EvidenceReport` 数据结构（字段覆盖：hit_id/identity/coverage/evalue/species/source_db/evidence_level 等，当前 internal backend 只填子集）
- `toxapp/evidence/report.py`：从内部索引生成 `EvidenceReport`（JSON 可序列化）
- `toxapp/evidence/tfidf.py`：基于训练集（带标签）构建 internal index，并对任意 query 批量生成“证据特征矩阵”（后续可作为 v2.2 的额外特征分支）
- `scripts/demo_internal_evidence_report_v3.py`：演示生成 evidence report（JSON）

说明：

- 当前环境未安装 `mmseqs2/jackhmmer/hmmersearch/blastp/diamond`，因此先用“内部训练集证据库”模拟证据检索（用于跑通 schema、网页接口与 Agent 报告流）。
- 你部署好外部工具与数据库后，下一步就是把 `EvidenceHit` 的 `identity/coverage/evalue/species/protein_name/annotations/evidence_level/source_db` 填满（见 6.1 的落地方案）。

### 1.2 MSA/profile：A3M 解析 + MSA summary 特征（v2.2 轻量路线）

已新增模块：

- `toxapp/msa/a3m.py`：A3M 读取与标准化（去 insertion，小写插入、`.`→`-` 等）
- `toxapp/msa/features.py`：
  - `msa_summary_features()`：输出 9 维 summary（depth、gap rate、entropy 等）
  - `profile_frequencies()`：输出 `[L,20]` 残基频率（为 v2.3 per‑residue 分支预留）

### 1.3 v3 可跑通训练/评估：在 v2 cache 基础上接入 MSA summary

新增 v3 产物：

- `toxapp/fusion_v3.py`：`EvidentialFusionV3`（在 v2 的 PLM+phys+motif 基础上加一条 `MSA summary`，以残差方式注入）
- `scripts/build_protein_toxdl2_feature_cache_v3.py`：
  - 基于 `data/feature_cache_v2/protein_toxdl2_v2` 复用 PLM/phys/motif
  - 新增 `msa_{split}.npy` 与 `seq_ids_{split}.txt`
  - 支持可选 `--msa-dir`：其中每条序列的 A3M 命名为 `<sha1[:16]>.a3m`
- `scripts/test_protein_plan_v3_toxdl2.py`：训练/评估 v3（并额外保存 `retrieval_tfidf.joblib` 用于 evidence report）

本轮已跑通（验证 end‑to‑end，而非追求最优指标）：

- v3 cache：`data/feature_cache_v3/protein_toxdl2_v3/meta.json`
  - 当前 `--msa-dir` 为空，因此 MSA summary 全为 0（meta 中会记录 missing 统计）
- v3 训练/评估（1 epoch smoke test）：`artifacts/protein_plan_test_v3_toxdl2_trilinear/summary.json`
- evidence report demo：可直接用 `artifacts/protein_plan_test_v3_toxdl2_trilinear/retrieval_tfidf.joblib` 生成结构化 JSON

---

## 2. 复现命令（当前环境：无外部同源检索工具）

### 2.1（已有）构建 v2 特征缓存

```bash
python scripts/build_protein_toxdl2_feature_cache_v2.py \
  --plm-path /root/group_data/qiuleyu/esm2_t12_35M_UR50D \
  --out-dir data/feature_cache_v2/protein_toxdl2_v2
```

### 2.2 构建 v3 特征缓存（先不提供 A3M，也能跑通）

```bash
python scripts/build_protein_toxdl2_feature_cache_v3.py \
  --base-cache-dir data/feature_cache_v2/protein_toxdl2_v2 \
  --out-dir data/feature_cache_v3/protein_toxdl2_v3 \
  --msa-dir ''
```

> 之后你部署好 `mmseqs2/hmmer` 并生成 A3M 后，把 `--msa-dir /path/to/a3m_dir` 指向包含 `<sha1[:16]>.a3m` 的目录即可自动补齐 MSA summary。

### 2.3 训练/评估 v3（可先跑 1 epoch 验证流程）

```bash
python scripts/test_protein_plan_v3_toxdl2.py \
  --cache-dir data/feature_cache_v3/protein_toxdl2_v3 \
  --fusion trilinear \
  --epochs 1 --patience 1 --batch-size 128 \
  --adapter-dim 32 --head-hidden 128
```

### 2.4 生成 evidence report（JSON，可给网页/Agent）

```bash
python scripts/demo_internal_evidence_report_v3.py \
  --index artifacts/protein_plan_test_v3_toxdl2_trilinear/retrieval_tfidf.joblib \
  --sequence 'YOUR_SEQUENCE' \
  --top-k 16 \
  --out-json artifacts/evidence_report_demo.json
```

---

## 3. 规划（按你给的 6.1–6.4；并标注当前完成情况）

### 6.1 优先级 A（强烈建议先做）：同源检索 → 证据库 → MSA/profile 特征

这条路线的优势：**完全不需要 3D 结构预测**，但能同时增强泛化、解释与“证据可追溯性”。也最符合你想做的网站/Agent 报告。

#### 6.1.1 同源检索与证据落地（可机读）

建议工具（按常用与可扩展排序）：

- `mmseqs2`：速度快、可大规模；适合构建你自己的“证据库/相似序列库”。
- `jackhmmer` / `hmmersearch`：擅长远同源；适合构建 MSA/profile。
- `blastp` / `diamond`：作为对照或快速 baseline。

证据库建议来源（优先可离线镜像/可下载的）：

- UniProtKB（Swiss‑Prot/TrEMBL）：用于“功能注释 + 实验/推断证据等级”。
- Tox‑Prot（UniProt 的 toxin subset）：用于高置信“毒素/毒蛋白”正例证据与注释字段。
- （可选）毒素/毒力相关数据库（如 VFDB 等）：用于扩展覆盖，但要更严格控制标签噪声。

建议输出一套统一证据 schema（用于网页/Agent）：

- query 信息：序列、长度、预测 `p_toxic/uncertainty`、所用模型版本
- hit 列表：hit_id、identity、coverage、evalue、物种、毒素相关关键词/注释、数据库来源、证据等级
- 证据汇总：top‑k 命中分布、是否命中 Tox‑Prot、命中一致性（预测与 hit 标签是否一致）

**v3 完成情况（当前能跑通）**

- 已实现统一 schema：`toxapp/evidence/schema.py`
- 已实现 internal 证据检索（用于先跑通网页/Agent）：`toxapp/evidence/report.py` + `scripts/demo_internal_evidence_report_v3.py`
- 待你部署工具后可替换：把 mmseqs2/hmmer 的输出解析成 `EvidenceHit`（并补齐 `identity/coverage/evalue/species/protein_name/evidence_level/source_db`）

#### 6.1.2 MSA/profile 特征（对齐 SCANS 思路）

从同源检索结果构建 MSA 后，可以抽取两类信息：

- **逐位点 profile**：PSSM（20 维）、保守性/熵、gap 率、MSA depth 等（解释性非常好）
- **序列级 summary**：平均保守性、低复杂区占比、MSA 深度分位数等（轻量、好部署）

把 profile 特征接入模型的方式（两档）：

- v2.2（轻量）：只用序列级 summary（直接 concat 到 v2 三路特征）
- v2.3（更强）：逐位点 profile 作为第四路“per‑residue 分支”，与 PLM residue embedding 做 cross‑attn 或轻 CNN 融合，再输出位点解释（类似 SCANS 的多分支思想）

**v3 完成情况（当前能跑通）**

- 已实现 A3M 解析 + 9 维 MSA summary：`toxapp/msa/a3m.py`、`toxapp/msa/features.py`
- 已实现 v2.2 的数据管线与模型注入（即使没有 A3M，也能跑通，特征为 0）：  
  - cache：`scripts/build_protein_toxdl2_feature_cache_v3.py`  
  - 模型：`toxapp/fusion_v3.py`  
  - 训练/评估：`scripts/test_protein_plan_v3_toxdl2.py`
- v2.3（逐位点 profile + cross‑attn 融合）尚未实现，但 `profile_frequencies()` 已预留接口

### 6.2 优先级 B（不依赖结构预测）：从 PLM 内部再挖一层“pseudo‑structure / pseudo‑annotation”

你已经有 v1 的 `PLM-contact pseudo-structure`，这里还有几个可继续提升的方向：

- **边特征更丰富**：把 `attention score`、`APC 后分位数`、`|i-j|`（序列距离）、是否同一窗口（长序列 chunk）等作为 edge features，而不是只有 edge_index。
- **图编码器更贴任务**：从 mean aggregation 升级到 edge-aware 的 message passing（仍可保持 2–3 层、hidden=128/256），以提高对“关键长程相互作用”的建模。
- **位点解释更稳**：把“pseudo-contact 图中心性/度”+“motif 命中位置”+“PLM residue saliency（梯度/遮挡）”合并成一套位点热图输出，提升可解释性一致性。

**v3 完成情况**

- 现有能力仍在（v1）：`toxapp/plm_contact.py` + `toxapp/graph.py`（当前不含 edge features；后续按上述三点迭代）

### 6.3 优先级 C（可选，离线增强）：xfold/快速折叠 → 几何图 → GeoGNN/几何模型（作为 teacher 或增强分支）

如果你愿意引入少量结构（强调：建议先离线、再迁移到轻量主线），优化空间依然很大：

#### 6.3.1 为什么 GeoGNN 有用、但不建议一上来就“全量在线依赖结构”

- GeoGNN/几何 GNN 的优势来自可靠几何输入（距离/角度/局部坐标系）。
- 但“在线每条序列都折叠”会把系统变成结构预测产品，违背你要的便捷性；同时结构误差会把噪声注入训练。

因此更推荐的定位：

- **离线 teacher / 对照**：在你有结构的子集上训练/评估几何模型，得到更强的判别或更可靠的位点解释；
- **知识迁移到轻量分支**：把 teacher 的信号转成可学习目标（例如：pseudo-contact 边权回归、位点热图对齐、ranking consistency），从而提升不依赖结构的主模型。

#### 6.3.2 “结构增强但仍轻量”的落地方式（两档）

- v2.4（保守）：只对一部分样本跑 xfold，抽取简单结构统计（接触密度、二级结构比例、局部几何分布）作为额外 feature；线上不需要结构。
- v2.5（更强）：对结构子集构建 residue graph（距离阈值边 + 角度/距离边特征），训练 GeoGNN；把输出的位点重要性/图表示用于蒸馏到 `PLM-contact` 分支。

### 6.4 与当前 v2/v3 的直接衔接（建议的实施顺序）

按“收益/成本/不破坏轻量主线”的优先级排序：

1) **先做证据检索与 schema 固化**：把 mmseqs2/hmmer 的 hit 结果写成统一 JSON，并接入网页与 Agent 报告（收益最大）。  
   - v3 已先用 internal TF‑IDF 跑通 schema/JSON，等待你部署真实工具后替换 backend。
2) **再做 MSA summary 特征**：先把 MSA depth/entropy 这类轻量 summary 接到 v2/v3（无需改太多网络）。  
   - v3 已跑通（A3M 可选输入）。
3) **升级 pseudo-contact 图分支**：补 edge features + 更强但仍轻量的图编码器，目标是至少追平/超过 v1 contact 的 AUPRC。
4) **最后再引入 xfold/GeoGNN（离线）**：用于 teacher/迁移与“解释可信度”提升，而不是让线上强依赖结构预测。

