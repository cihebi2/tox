# 毒性预测项目计划 v3（证据检索 + MSA/Profile + pseudo‑structure；先跑通后逐步替换为真实同源检索）

更新时间：2026-01-16 20:33:49 +0800

目标：把你在 v2 里提出的“**证据可追溯**（可机读 hit 列表）+ **MSA/profile 特征**（提升泛化/解释）”先落到一个**可运行的最小框架**里；当前环境里尚未部署 `mmseqs2/hmmer/blast/diamond` 与外部大库，所以先用 **internal TF‑IDF evidence** 与 **可选 A3M 输入**把管线跑通，之后你部署好工具与库后再把 backend 切换到真实同源检索。

---

## 1. 本轮交付（已实现且可跑通）

### 1.1 证据检索：统一 schema + internal TF‑IDF evidence（可机读）

已新增模块（用于网页/Agent 报告的结构化中间产物）：

- `toxapp/evidence/schema.py`：`EvidenceQuery/EvidenceHit/EvidenceSummary/EvidenceReport` 数据结构（字段覆盖：hit_id/identity/coverage/evalue/species/source_db/evidence_level 等，当前 internal backend 只填子集）
- `toxapp/evidence/report.py`：从内部索引生成 `EvidenceReport`（JSON 可序列化）
- `toxapp/evidence/tfidf.py`：基于训练集（带标签）构建 internal index，并对任意 query 批量生成“证据特征矩阵”（后续可作为 v2.2 的额外特征分支）
- `scripts/demo_internal_evidence_report_v3.py`：演示生成 evidence report（JSON）

说明：

- 当前环境未安装 `mmseqs2/jackhmmer/hmmersearch/blastp/diamond`，因此先用“内部训练集证据库”模拟证据检索（用于跑通 schema、网页接口与 Agent 报告流）。
- 你部署好外部工具与数据库后，下一步就是把 `EvidenceHit` 的 `identity/coverage/evalue/species/protein_name/annotations/evidence_level/source_db` 填满（见 6.1 的落地方案）。

### 1.2 MSA/profile：A3M 解析 + MSA summary 特征（v2.2 轻量路线）

已新增模块：

- `toxapp/msa/a3m.py`：A3M 读取与标准化（去 insertion，小写插入、`.`→`-` 等）
- `toxapp/msa/features.py`：
  - `msa_summary_features()`：输出 9 维 summary（depth、gap rate、entropy 等）
  - `profile_frequencies()`：输出 `[L,20]` 残基频率（为 v2.3 per‑residue 分支预留）

### 1.3 v3 可跑通训练/评估：在 v2 cache 基础上接入 MSA summary

新增 v3 产物：

- `toxapp/fusion_v3.py`：`EvidentialFusionV3`（在 v2 的 PLM+phys+motif 基础上加一条 `MSA summary`，以残差方式注入）
- `scripts/build_protein_toxdl2_feature_cache_v3.py`：
  - 基于 `data/feature_cache_v2/protein_toxdl2_v2` 复用 PLM/phys/motif
  - 新增 `msa_{split}.npy` 与 `seq_ids_{split}.txt`
  - 支持可选 `--msa-dir`：其中每条序列的 A3M 命名为 `<sha1[:16]>.a3m`
- `scripts/test_protein_plan_v3_toxdl2.py`：训练/评估 v3（并额外保存 `retrieval_tfidf.joblib` 用于 evidence report）

本轮已跑通（验证 end‑to‑end，而非追求最优指标）：

- v3 cache：`data/feature_cache_v3/protein_toxdl2_v3/meta.json`
  - 当前 `--msa-dir` 为空，因此 MSA summary 全为 0（meta 中会记录 missing 统计）
- v3 训练/评估（1 epoch smoke test）：`artifacts/protein_plan_test_v3_toxdl2_trilinear/summary.json`
- evidence report demo：可直接用 `artifacts/protein_plan_test_v3_toxdl2_trilinear/retrieval_tfidf.joblib` 生成结构化 JSON

---

## 2. 复现命令（当前环境：无外部同源检索工具）

### 2.1（已有）构建 v2 特征缓存

```bash
python scripts/build_protein_toxdl2_feature_cache_v2.py \
  --plm-path /root/group_data/qiuleyu/esm2_t12_35M_UR50D \
  --out-dir data/feature_cache_v2/protein_toxdl2_v2
```

### 2.2 构建 v3 特征缓存（先不提供 A3M，也能跑通）

```bash
python scripts/build_protein_toxdl2_feature_cache_v3.py \
  --base-cache-dir data/feature_cache_v2/protein_toxdl2_v2 \
  --out-dir data/feature_cache_v3/protein_toxdl2_v3 \
  --msa-dir ''
```

> 之后你部署好 `mmseqs2/hmmer` 并生成 A3M 后，把 `--msa-dir /path/to/a3m_dir` 指向包含 `<sha1[:16]>.a3m` 的目录即可自动补齐 MSA summary。

### 2.3 训练/评估 v3（可先跑 1 epoch 验证流程）

```bash
python scripts/test_protein_plan_v3_toxdl2.py \
  --cache-dir data/feature_cache_v3/protein_toxdl2_v3 \
  --fusion trilinear \
  --epochs 1 --patience 1 --batch-size 128 \
  --adapter-dim 32 --head-hidden 128
```

### 2.4 生成 evidence report（JSON，可给网页/Agent）

```bash
python scripts/demo_internal_evidence_report_v3.py \
  --index artifacts/protein_plan_test_v3_toxdl2_trilinear/retrieval_tfidf.joblib \
  --sequence 'YOUR_SEQUENCE' \
  --top-k 16 \
  --out-json artifacts/evidence_report_demo.json
```

---

## 3. 规划（按你给的 6.1–6.4；并标注当前完成情况）

### 6.1 优先级 A（强烈建议先做）：同源检索 → 证据库 → MSA/profile 特征

这条路线的优势：**完全不需要 3D 结构预测**，但能同时增强泛化、解释与“证据可追溯性”。也最符合你想做的网站/Agent 报告。

#### 6.1.1 同源检索与证据落地（可机读）

建议工具（按常用与可扩展排序）：

- `mmseqs2`：速度快、可大规模；适合构建你自己的“证据库/相似序列库”。
- `jackhmmer` / `hmmersearch`：擅长远同源；适合构建 MSA/profile。
- `blastp` / `diamond`：作为对照或快速 baseline。

证据库建议来源（优先可离线镜像/可下载的）：

- UniProtKB（Swiss‑Prot/TrEMBL）：用于“功能注释 + 实验/推断证据等级”。
- Tox‑Prot（UniProt 的 toxin subset）：用于高置信“毒素/毒蛋白”正例证据与注释字段。
- （可选）毒素/毒力相关数据库（如 VFDB 等）：用于扩展覆盖，但要更严格控制标签噪声。

建议输出一套统一证据 schema（用于网页/Agent）：

- query 信息：序列、长度、预测 `p_toxic/uncertainty`、所用模型版本
- hit 列表：hit_id、identity、coverage、evalue、物种、毒素相关关键词/注释、数据库来源、证据等级
- 证据汇总：top‑k 命中分布、是否命中 Tox‑Prot、命中一致性（预测与 hit 标签是否一致）

**v3 完成情况（当前能跑通）**

- 已实现统一 schema：`toxapp/evidence/schema.py`
- 已实现 internal 证据检索（用于先跑通网页/Agent）：`toxapp/evidence/report.py` + `scripts/demo_internal_evidence_report_v3.py`
- 待你部署工具后可替换：把 mmseqs2/hmmer 的输出解析成 `EvidenceHit`（并补齐 `identity/coverage/evalue/species/protein_name/evidence_level/source_db`）

#### 6.1.2 MSA/profile 特征（对齐 SCANS 思路）

从同源检索结果构建 MSA 后，可以抽取两类信息：

- **逐位点 profile**：PSSM（20 维）、保守性/熵、gap 率、MSA depth 等（解释性非常好）
- **序列级 summary**：平均保守性、低复杂区占比、MSA 深度分位数等（轻量、好部署）

把 profile 特征接入模型的方式（两档）：

- v2.2（轻量）：只用序列级 summary（直接 concat 到 v2 三路特征）
- v2.3（更强）：逐位点 profile 作为第四路“per‑residue 分支”，与 PLM residue embedding 做 cross‑attn 或轻 CNN 融合，再输出位点解释（类似 SCANS 的多分支思想）

**v3 完成情况（当前能跑通）**

- 已实现 A3M 解析 + 9 维 MSA summary：`toxapp/msa/a3m.py`、`toxapp/msa/features.py`
- 已实现 v2.2 的数据管线与模型注入（即使没有 A3M，也能跑通，特征为 0）：  
  - cache：`scripts/build_protein_toxdl2_feature_cache_v3.py`  
  - 模型：`toxapp/fusion_v3.py`  
  - 训练/评估：`scripts/test_protein_plan_v3_toxdl2.py`
- v2.3（逐位点 profile + cross‑attn 融合）尚未实现，但 `profile_frequencies()` 已预留接口

### 6.2 优先级 B（不依赖结构预测）：从 PLM 内部再挖一层“pseudo‑structure / pseudo‑annotation”

你已经有 v1 的 `PLM-contact pseudo-structure`，这里还有几个可继续提升的方向：

- **边特征更丰富**：把 `attention score`、`APC 后分位数`、`|i-j|`（序列距离）、是否同一窗口（长序列 chunk）等作为 edge features，而不是只有 edge_index。
- **图编码器更贴任务**：从 mean aggregation 升级到 edge-aware 的 message passing（仍可保持 2–3 层、hidden=128/256），以提高对“关键长程相互作用”的建模。
- **位点解释更稳**：把“pseudo-contact 图中心性/度”+“motif 命中位置”+“PLM residue saliency（梯度/遮挡）”合并成一套位点热图输出，提升可解释性一致性。

**v3 完成情况**

- 现有能力仍在（v1）：`toxapp/plm_contact.py` + `toxapp/graph.py`（当前不含 edge features；后续按上述三点迭代）

### 6.3 优先级 C（可选，离线增强）：xfold/快速折叠 → 几何图 → GeoGNN/几何模型（作为 teacher 或增强分支）

如果你愿意引入少量结构（强调：建议先离线、再迁移到轻量主线），优化空间依然很大：

#### 6.3.1 为什么 GeoGNN 有用、但不建议一上来就“全量在线依赖结构”

- GeoGNN/几何 GNN 的优势来自可靠几何输入（距离/角度/局部坐标系）。
- 但“在线每条序列都折叠”会把系统变成结构预测产品，违背你要的便捷性；同时结构误差会把噪声注入训练。

因此更推荐的定位：

- **离线 teacher / 对照**：在你有结构的子集上训练/评估几何模型，得到更强的判别或更可靠的位点解释；
- **知识迁移到轻量分支**：把 teacher 的信号转成可学习目标（例如：pseudo-contact 边权回归、位点热图对齐、ranking consistency），从而提升不依赖结构的主模型。

#### 6.3.2 “结构增强但仍轻量”的落地方式（两档）

- v2.4（保守）：只对一部分样本跑 xfold，抽取简单结构统计（接触密度、二级结构比例、局部几何分布）作为额外 feature；线上不需要结构。
- v2.5（更强）：对结构子集构建 residue graph（距离阈值边 + 角度/距离边特征），训练 GeoGNN；把输出的位点重要性/图表示用于蒸馏到 `PLM-contact` 分支。

### 6.4 与当前 v2/v3 的直接衔接（建议的实施顺序）

按“收益/成本/不破坏轻量主线”的优先级排序：

1) **先做证据检索与 schema 固化**：把 mmseqs2/hmmer 的 hit 结果写成统一 JSON，并接入网页与 Agent 报告（收益最大）。  
   - v3 已先用 internal TF‑IDF 跑通 schema/JSON，等待你部署真实工具后替换 backend。
2) **再做 MSA summary 特征**：先把 MSA depth/entropy 这类轻量 summary 接到 v2/v3（无需改太多网络）。  
   - v3 已跑通（A3M 可选输入）。
3) **升级 pseudo-contact 图分支**：补 edge features + 更强但仍轻量的图编码器，目标是至少追平/超过 v1 contact 的 AUPRC。
4) **最后再引入 xfold/GeoGNN（离线）**：用于 teacher/迁移与“解释可信度”提升，而不是让线上强依赖结构预测。

---

## 7. 外部工具与硬盘预算（部署预估）

更新时间：2026-01-17 14:02:43 +0800

结论先说：**工具本体几乎不占空间**；硬盘主要消耗在 **（1）同源检索用序列库**、**（2）检索索引**、**（3）离线注释库/映射表（可选）**、**（4）A3M/MSA 缓存（可选）**。

### 7.1 v3 优先级 A 所需：同源检索 + MSA/profile（不做结构预测）

#### 7.1.1 工具本体（仅安装）

- `mmseqs2`：通常 < 1GB（二进制/依赖）
- `hmmer`（`jackhmmer/hmmersearch/hmmscan`）：通常 < 1GB
- `blastp`（NCBI BLAST+）/ `diamond`：通常 < 1GB（作为 baseline/对照可选）

> 若用 conda 单独建环境，建议额外预留 `2–5GB`（依赖包体 + cache）。

#### 7.1.2 核心“吃硬盘”的数据：UniProt/UniRef（下载包体，gz）

以下为 UniProt FTP（`current_release`）常用文件的**压缩包**体积（仅供预算；实际会随 release 变化）：

| 组件（gz） | 用途 | 体积（约） |
|---|---:|---:|
| `uniprot_sprot.fasta.gz` | 高质量 Swiss‑Prot（证据/注释更干净） | `0.09GB` |
| `uniprot_trembl.fasta.gz` | TrEMBL（覆盖更大但更噪） | `48GB` |
| `uniref50.fasta.gz` | 推荐的“轻量检索/MSA”起步库 | `12GB` |
| `uniref90.fasta.gz` | 覆盖更强的检索/MSA 库 | `43GB` |
| `uniref100.fasta.gz` | 更全但更大 | `116GB` |
| `idmapping_selected.tab.gz`（可选） | 离线映射/部分注释增强 | `9GB` |

（更重的“全离线注释”可选项，通常不建议 v3 一开始就上）

| 组件（gz） | 用途 | 体积（约） |
|---|---:|---:|
| `uniprot_sprot.dat.gz` / `uniprot_sprot.xml.gz` | 离线 Swiss‑Prot 注释解析 | `0.65–0.88GB` |
| `uniprot_trembl.dat.gz` / `uniprot_trembl.xml.gz` | 离线 TrEMBL 全注释解析 | `139–171GB` |

#### 7.1.3 典型落地体积：库 + 索引 + 缓存（经验范围）

> 关键点：**同一份 FASTA** 通常会同时保留 `压缩包(.gz)`、`解压后的 FASTA`、以及 `mmseqs2 索引库`；因此落地占用会远大于 `.gz`。

- `mmseqs2 createdb + createindex` 后：常见总占用约为 `1.3–2.0×（解压后 FASTA 体积）`（与参数/版本/索引选项有关）。
- `A3M/MSA 缓存`（给训练集/高频查询复用）：通常额外 `10–100GB`（取决于 MSA 深度、序列长度、是否全量缓存）。
- `scratch 临时盘`（建库/索引/批量检索时强烈建议）：预留 `50–200GB` 更稳。

#### 7.1.4 推荐三档硬盘预算（从“先跑通”到“更接近生产”）

1) **档 A：先跑通（推荐）**  
   - 工具：`mmseqs2 + hmmer`  
   - 数据库：只用 `UniRef50`（检索/MSA）+（可选）`uniprot_sprot.fasta.gz`（更干净的证据注释源）  
   - 预算：约 `150–400GB`（含索引/原始包/少量缓存）

2) **档 B：覆盖更强（常见线上配置）**  
   - 数据库：`UniRef90` +（可选）`UniProtKB FASTA（sprot+trembl）` +（可选）`idmapping_selected`  
   - 预算：约 `600–1000GB`

3) **档 C：全离线注释（重，不建议 v3 起步）**  
   - 额外引入 `uniprot_trembl.dat/xml` 等，做本地注释解析与索引  
   - 预算：通常 `1–2TB+`

### 7.2 其他可选外部工具（按“解释/增强”价值排序）

这些不是 v3 必需，但你后续想把“证据解释/远同源/功能注释”做得更强时会用到：

- **HH-suite（`hhblits/hhsearch`）**：工具本体小（<1GB），但常用库（如 `Uniclust30/UniRef30`）通常是**几十到上百 GB**级；越全越大。
- **PSI‑BLAST（PSSM/profile）**：工具小，但依赖 `nr` 这类库，常见是**数百 GB**甚至更大（并且需要 BLAST DB 格式与索引）。
- **Pfam（域注释，`hmmscan + Pfam-A`）**：数据库本身属于“中等体量”，通常可按 **`5–10GB`** 级别预算（含解压与 `hmmpress`）。
- **InterProScan / eggNOG‑mapper（功能注释更全，但偏重）**：通常从**几十 GB 起**，完整配置可能到 **`50–100GB+`**（与数据库组件选择强相关）。
- **ColabFold 的 search 数据库（仅用于检索/MSA，不一定折叠）**：通常在 **`100–200GB`** 级别（随组合与版本变化）。

### 7.3 结构预测相关（v3 里定位为“离线可选 teacher”，不建议作为线上强依赖）

- 若只考虑“少量结构子集离线增强”，常见单序列折叠模型的**权重**多在 `~1–10GB` 级别；真正的成本更多是 GPU 时间，而不是硬盘。
- 若走 AlphaFold2 全量数据库路线，硬盘会进入 **TB 级（常见 2TB+）**，与 v3 的“轻量可部署”目标不一致，因此建议仅作为“离线对照/teacher”。

### 7.4 在 `≤200GB` 约束下的推荐组合（准确率 + 报告可靠性优先）

更新时间：2026-01-17 14:20:08 +0800

在你当前 v3 架构（证据检索 + MSA/profile + PLM）下，建议优先保证“可做同源检索/MSA”的能力，其次补齐“高置信注释源”，最后再加轻量域注释：

- **必须（提升准确率/泛化最直接）：一个可做同源检索/MSA 的序列库**
  - 首选：`UniRef50`（给 `mmseqs2` / `jackhmmer` 用）
  - 预算：通常做到“可检索 + 索引 + 少量缓存”会落在 `~120–180GB`（取决于是否保留 `.gz/解压 FASTA/中间文件`、以及 `mmseqs2` 索引参数）
- **必须（提升报告可靠性最直接）：一个“高置信注释源”用来解释命中与证据等级**
  - 首选：`UniProt Swiss‑Prot`（离线只要 `uniprot_sprot.fasta.gz` + 可选 `uniprot_sprot.dat.gz/xml.gz`）
  - 预算：`<1GB` 级；避免 TrEMBL 的 `.dat/.xml`（体量过大，不符合 200GB 约束）
- **强烈建议但可选（让报告更像“生物学证据”而非纯相似性）：`Pfam-A`（域注释）**
  - 预算：通常 `~5–10GB`（含解压与 `hmmpress`）
