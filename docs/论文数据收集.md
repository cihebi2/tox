# 论文数据收集（检索→全文/补充材料→结构化抽取）流程规划

更新时间：2026-01-17 06:17:40 +0800

更新记录：
- 2026-01-16 11:19:52 +0800：修复并启用 PMC Open Access tar.gz 回退（`oa.fcgi` → `oa_package/*.tar.gz`），当 OA PDF 直链/PMC instance 补充材料下载失败（常见 403/PoW）时自动尝试；已在 `papers_oa_test5/` 验证 MDPI/CSBJ 可成功获取主文 PDF 与补充文件。
- 2026-01-16 12:16:41 +0800：新增“结构化抽取”脚本 `scripts/extract_paper_extractions.py`，将下载产物（XML/PDF/补充材料）抽取为可审计的 `raw_extractions.csv`，并把表格落盘为 `extracted_tables/*.csv`。
- 2026-01-16 14:37:08 +0800：抽取脚本新增 `raw_experimental_records.csv`（把“同一表格行”的 `sequence` 与 `endpoint value` 关联，带 `table_id/supp_file` 指针）；并增强 `.docx` 表格解析与 PDF 阈值句抽取。
- 2026-01-16 16:54:02 +0800：新增“实验数据优先”发现脚本 `scripts/discover_epmc_experimental_papers.py`；并将 `scripts/extract_paper_extractions.py` 升级为**跨表关联**（`peptide_id/name ↔ sequence` + `peptide_id/name ↔ endpoint value`），支持 rowspan/colspan、双层表头、`MIC (MBC)` 单元格拆分、`hemolysis (%)` 列抽取；`raw_experimental_records.csv` 现包含 `peptide_id/sequence_raw/condition` 等列，且 `source_ref` 使用 `row_idx/col_idx/header_row_idx` 便于定位复核。
- 2026-01-16 23:02:37 +0800：完成 10 篇 OA 试运行（`runs/oa10_toxicity_seq/papers/`），输出 `raw_experimental_records.csv` + `extraction_report.md`；并修复“端点写在 `Parameter` 列”的表格抽取（避免把 `MIC/MBC` 误标成 `IC50`）。
- 2026-01-17 00:12:21 +0800：完成 50 篇 OA 候选检索（Europe PMC，优先“可机器抽取表格”的序列+端点），输出候选列表 `runs/oa50_candidates.csv` 与可直接下载的 id 清单 `runs/oa50_candidate_ids.txt`；并额外导出“毒性端点优先”子集 `runs/oa50_candidates_toxfocus.csv` / `runs/oa50_candidate_ids_toxfocus.txt`；发现脚本新增 `--max-candidates` 便于按数量提前停止扫描。
- 2026-01-17 06:17:40 +0800：抽取端点时过滤“比值/指数列”（如 `SI (CC50/IC50)`、`CC50/IC50`），避免把衍生指标误标成真实端点；已重新生成 `runs/oa10_toxicity_seq/papers/` 的 `raw_experimental_records.csv` 与 `extraction_report.md`。

目标：用**可复现、可合规**的方式，批量收集“与毒性/毒肽/毒素蛋白相关”的论文全文与补充材料，并沉淀为后续可抽取的结构化数据源（序列、端点、实验条件、数值标签、引用）。

本规划默认优先覆盖 **Open Access（OA）** 论文；对非 OA 论文，脚本不会绕过付费墙，需要你具备机构订阅/出版社 TDM 授权后再接入对应渠道。

---

## 1. 推荐流程（先跑通闭环，再扩大规模）

### 1.1 先做“论文清单”

建立一个文本文件 `paper_ids.txt`（每行一个标识符，支持 DOI/PMID/PMCID/arXiv/URL）。例如：

```
10.1186/s12859-024-05748-z
10.3390/toxins17100489
10.1016/j.csbj.2025.05.039
```

建议你先用 10–30 篇作为试运行集合，确认输出目录结构、日志、补充材料抓取策略满足预期，再扩大到数百/数千。

### 1.2 批量下载（只抓 OA）

使用脚本（见第 2 节）下载：

- PDF：用于人工阅读/归档
- JATS XML（若在 PMC/Europe PMC 可得）：用于后续自动抽取（表格、补充材料链接、方法字段）
- Supplementary/Additional files：优先通过 XML 中的 `supplementary-material` 提取（例如 BMC/Springer 的 MOESM 文件）；若无 XML，则会对 landing page 做一次“轻量链接扫描”（不保证命中，且不绕过任何反爬/付费墙）

### 1.3 人工补齐（对“非 OA / 无法自动发现补充材料”的论文）

脚本会在 manifest 中标注失败原因（无 OA、无 PMCID、没有可下载的 supp 链接等）。对这些论文你有三种合规路径：

1) 学校/研究所图书馆订阅（浏览器登录后手动下载）  
2) 预印本（bioRxiv/medRxiv/arXiv/chemRxiv）  
3) 联系作者索取补充表/原始测量数据（尤其序列表 Excel/FASTA）

---

## 2. 批量获取脚本（OA-only）

脚本位置：`/root/private_data/aa_peptide/toxin/docs/scripts/fetch_open_access_papers.py`

### 2.1 需要的权限 / Token

**不需要 token** 也能跑通（使用 OpenAlex + Europe PMC）。

可选增强（推荐）：设置 Unpaywall 的 email（不是 token，用于合规统计与限流）：

```bash
export UNPAYWALL_EMAIL="you@lab.edu"
```

说明：

- Unpaywall：需要 `email` 参数；可显著提高“找到 OA PDF 直链”的成功率
- OpenAlex：不需要 key；但仍建议自定义 `--user-agent` 标明用途
- Europe PMC：不需要 key；可直接获取 PMC 收录 OA 文章的 `fullTextXML`

如果你后续希望**程序化下载非 OA 的出版社 PDF**：通常需要机构级别的 TDM 授权 + 出版社 API Key（例如 Elsevier TDM、Springer Nature TDM 等），这不在本次 OA-only 脚本范围内。

### 2.2 输出目录建议

建议把输出统一放到：

- `docs/papers_oa/`（本轮测试）
- 每次运行都会追加写入 `download_manifest.jsonl`，记录每个 id 的解析来源、下载链接、状态、sha256
- 开启 `--supplementary` 时，会输出 `supplementary/<paper_stem>/`；必要时会输出 `pmc_oa_packages/<paper_stem>.tar.gz`（可用 `--no-pmc-oa-package` 关闭）

---

## 3. 试运行（我们将立即执行）

### 3.1 试运行集合（优先 OA）

先用以下 DOI 做一次小规模下载（覆盖 BMC/MDPI/CSBJ/ACS 等不同出版社，自动下载会“能下则下，不能下则记录原因”）：

- 10.1186/s12859-024-05748-z（BMC Bioinformatics，OA，且在 PMC，适合测试补充材料抓取）
- 10.3390/toxins17100489（MDPI，通常 OA）
- 10.1016/j.csbj.2025.04.002（CSBJ，通常 OA）
- 10.1016/j.csbj.2025.05.039（CSBJ，通常 OA）
- 10.1021/acs.jcim.5c02518（ACS，通常非 OA：用于验证脚本会正确跳过而不“硬下”）

### 3.2 执行命令

在 `docs/` 下执行：

```bash
mkdir -p papers_oa
cat > paper_ids.txt <<'EOF'
10.1186/s12859-024-05748-z
10.3390/toxins17100489
10.1016/j.csbj.2025.04.002
10.1016/j.csbj.2025.05.039
10.1021/acs.jcim.5c02518
EOF

python scripts/fetch_open_access_papers.py --input paper_ids.txt --out papers_oa --format both
python scripts/fetch_open_access_papers.py --input paper_ids.txt --out papers_oa --format both --supplementary
```

运行完成后，查看：

- `papers_oa/download_manifest.jsonl`：每条的下载状态/来源/sha256
- `papers_oa/*.pdf` / `papers_oa/*.xml`：成功下载的文件

---

## 4. 下一步（把“下载”连到“自动抽取”）

当下载侧跑通后，使用结构化抽取脚本将“论文文件 → 可复核的结构化记录”。

脚本位置：`scripts/extract_paper_extractions.py`

推荐命令（以 `papers_oa/` 为例）：

```bash
python scripts/extract_paper_extractions.py --input-dir papers_oa --overwrite
```

输出（默认写入到 `papers_oa/` 下）：

- `raw_extractions.csv`：逐条抽取记录，包含 DOI/PMID/PMCID + 来源指针（`table_id` / `supp_file` / `page` 等）
- `raw_experimental_records.csv`：行级“序列-端点值”记录（支持**跨表关联**：序列表 + 端点表分离也能拼接；每条记录带来源指针与表格 caption，用于审计）
- `extracted_tables/`：从 JATS XML / 补充表格重建出的 CSV（便于人工复核与二次解析）

说明：该抽取是“高召回 + 可审计”的 raw 抽取（不是最终清洗后的数据集）。如需控制长文本字段长度可用 `--max-field-len`，PDF 扫描页数用 `--max-pdf-pages`；如只想保留 `raw_extractions.csv` 可加 `--no-experimental`。

---

## 5. 实验数据优先（“一手序列+数值”）的推荐闭环

目标：优先拿到**原始实验测量值**（如 MIC/MBC、溶血%、CC50/IC50 等）并可追溯到论文表格/补充材料。

### 5.1 先发现候选论文（Europe PMC，OA-only）

脚本：`scripts/discover_epmc_experimental_papers.py`

示例（抗菌肽 + 溶血 + MIC/HC50/CC50/IC50，排除 Review）：

```bash
python scripts/discover_epmc_experimental_papers.py \
  --query '((hemolysis OR hemolytic OR erythrocyte) AND (antimicrobial peptide OR peptides) AND (MIC OR "minimum inhibitory concentration" OR HC50 OR CC50 OR IC50) AND (sequence OR sequences)) NOT PUB_TYPE:"Review"' \
  --max-results 50 \
  --min-score 5 \
  --out experimental_candidates_amp.csv \
  --ids-out experimental_candidate_ids_amp.txt
```

产物：
- `experimental_candidates_amp.csv`：候选列表（含 PMCID/DOI/打分原因）
- `experimental_candidate_ids_amp.txt`：可直接喂给下载脚本的 id 列表

### 5.2 下载全文 + 补充材料（OA-only）

```bash
python scripts/fetch_open_access_papers.py \
  --input experimental_candidate_ids_amp.txt \
  --out papers_oa_experimental_amp \
  --format both \
  --supplementary \
  --timeout 45 \
  --delay 0.5
```

### 5.3 抽取一手数据（序列 + 数值）

```bash
python scripts/extract_paper_extractions.py --input-dir papers_oa_experimental_amp --overwrite
```

重点输出：
- `papers_oa_experimental_amp/raw_experimental_records.csv`：序列-端点值（含 `peptide_id/sequence_raw/condition/source_ref/context`）
- `papers_oa_experimental_amp/extracted_tables/`：可复核的表格 CSV（行列与 `source_ref` 对齐）

---

## 6. 试运行结果（更新时间：2026-01-17 09:05 +0800）

本节记录“转置表 + MIC/MBC 2/2 拆分 + id/别名映射”增强后，重新抽取与批量抽取的实际产出。

### 6.1 已下载 10 篇（重新抽取）

- 运行目录：`runs/oa10_toxicity_seq/papers`
- 报告：`runs/oa10_toxicity_seq/papers/extraction_report.md`
- 一手实验数据：`runs/oa10_toxicity_seq/papers/raw_experimental_records.csv`
- 统计（来自报告）：
  - records：610
  - unique sequences：49
  - endpoints：MIC 268 / MBC 180 / IC50 98 / HEMOLYSIS 26 / HC50 11 / MHC 8 / LD50 8 / HC10 7 / CC50 4
- 示例（DOI `10.1186/s13071-025-06861-5`，Tab2）：已抽取 `CZS-1/CZS-11/CZS-7/CZS-5*` 各自的 `IC50` 与 `CC50`（并保留 `source_ref` 指向表格单元格）。

### 6.2 批量 50 篇 OA（下载 + 抽取）

- 运行目录：`runs/oa50_toxicity_seq/papers`
- 报告：`runs/oa50_toxicity_seq/papers/extraction_report.md`
- 一手实验数据：`runs/oa50_toxicity_seq/papers/raw_experimental_records.csv`
- 统计（来自报告）：
  - records：3564
  - unique sequences：100
  - endpoints：MIC 2931 / MBC 496 / IC50 58 / HEMOLYSIS 37 / HC10 13 / HC50 13 / MHC 8 / LD50 8

备注：部分论文仍可能为 0 records，常见原因是序列未以“可机读表格/补充表格”形式提供（只有图片或正文描述），或仅有计算/预测结果但无可 JOIN 的实验端点表。

---

## 7. 按“训练可用性”标准再筛查（更新时间：2026-01-17 10:50 +0800）

本节按以下“保守可比”标准对已下载 OA 文献的抽取结果做 QC 筛查，并输出可复核的标记数据表：

- 严格（`train_ready_strict`）：AA20 标准序列 + 有数值 + 有单位 + 非汇总/派生 + 非删失（无 `>`/`<`）
- 宽松（`train_ready_relaxed`）：允许删失值（如 `> 128`），但仍要求有单位 + 非汇总/派生（适合显式建模删失/区间）

脚本与产物：

- 脚本：`scripts/screen_experimental_records.py`
- OA10：
  - 报告：`runs/oa10_toxicity_seq/papers/screening_report.md`
  - 标记数据：`runs/oa10_toxicity_seq/papers/screened_experimental_records.csv`
- OA50：
  - 报告：`runs/oa50_toxicity_seq/papers/screening_report.md`
  - 标记数据：`runs/oa50_toxicity_seq/papers/screened_experimental_records.csv`

筛查摘要（来自报告）：

- OA10：
  - records 610；strict 400；relaxed 440
  - unit_missing 0；censored 70；aggregate 154
- OA50：
  - records 3564；strict 2993；relaxed 3237
  - unit_missing 64（主要集中在少数论文的 MIC/HC10/HC50 表头未给单位）；censored 280；aggregate 154

建议：后续训练/对比时，优先用 `train_ready_strict=1` 的子集作为“跨论文可比基准”；`train_ready_relaxed=1` 的子集用于“含删失值”的更大规模训练（配合区间/删失回归或特定标签编码）。

---

## 8. 数据收集流程（详细说明，更新时间：2026-01-17 11:22 +0800）

本项目的数据收集目标是：从 **OA（Open Access）文献**中获取“**一手实验数据**”，并尽量做到**可追溯、可复核、可用于后续建模**。

### 8.1 目标数据长什么样

最终希望沉淀为可训练的数据表（逐条记录）：

- `sequence`：肽/蛋白序列（尽量规范化为 AA 字母序列）
- `endpoint`：实验端点（如 `MIC/MBC/IC50/CC50/HC10/HC50/MHC/HEMOLYSIS/LD50`）
- `value` + `unit` + `cmp`：数值 + 单位 + 比较符号（如 `> 128` 的 `cmp=">"`）
- `condition`：实验对象/条件（菌株、细胞系、RBC 类型等，来自表头/行头）
- `source_ref`：指向来源单元格（table id + 行列索引）用于审计复核

对应到本仓库实际输出为：

- `raw_experimental_records.csv`：尽量做到“序列 × 端点 × 数值”的可追溯记录（训练原料）
- `extracted_tables/*.csv`：把论文表格（XML/补充材料）重建成 CSV，供人工复核

### 8.2 数据来源与获取原则（OA-only）

整个流程坚持 OA-only，不绕过付费墙：

- **Europe PMC**：用于检索候选、下载 JATS XML、获取 PMCID 与 PMC OA 包
- **PMC OA package**：当站点 PDF/补充材料下载不稳定时，用 PMC OA 包做兜底（可提取 PDF 与补充文件）
- **OpenAlex**：用于解析 OA 文章的落地 PDF 链接/开放版本信息（辅助下载）
- **Unpaywall（可选）**：如果设置了 `UNPAYWALL_EMAIL`（或 `--unpaywall-email`），可提升 OA PDF 解析命中率（仍只取 OA）

所有下载尝试、来源、sha256 与状态都会写入 `download_manifest.jsonl`，保证过程可审计。

### 8.3 端到端流程分解（发现 → 下载 → 抽取 → 筛查）

#### Step A：发现候选论文（带实验数据倾向的 OA 文献）

脚本：`scripts/discover_epmc_experimental_papers.py`

做法：

1) 向 Europe PMC 发起检索（query 由你指定，通常包含 “sequence + endpoint + assay” 关键词，并排除 Review）。
2) 对每条结果做轻量启发式打分（是否出现端点词、序列词、表格/补充材料线索等）。
3) 输出：
   - `*_candidates.csv`：候选清单（含 DOI/PMCID/PMID、打分、命中原因）
   - `*_candidate_ids.txt`：用于下一步下载的一列 paper ids（DOI/PMCID/PMID）

示例（见前文 5.1）：通过 “hemolysis + MIC/IC50 + sequence” 这类组合提高“有一手实验表格”的命中率。

#### Step B：下载全文与补充材料（OA-only）

脚本：`scripts/fetch_open_access_papers.py`

输入：`*_candidate_ids.txt`（一行一个 DOI/PMCID/PMID）

做法（对每个 id）：

1) 解析基础元信息（优先获取 DOI/PMCID，统一成 normalized id）。
2) 下载：
   - 主文 PDF（若可得）
   - JATS XML（尽量获取；Europe PMC/PMC 通常可提供）
   - 补充材料（若可得）：会尝试直接链接或从 PMC OA package 解包提取
3) 防卡死/防异常：
   - `--timeout`：单次 HTTP 超时
   - `--delay`：控制请求频率
   - `--max-file-mb` 与 `--max-file-seconds`：避免超大文件或卡住的下载
4) 输出目录结构（以 `<run>/papers` 为例）：
   - `download_manifest.jsonl`：每篇的动作日志（来源、状态、sha256、路径）
   - `<stem>.xml` / `<stem>.pdf`
   - `supplementary/<stem>/*`：解包后的补充文件

#### Step C：从论文中抽取“序列 + 端点数值”（可追溯 raw 抽取）

脚本：`scripts/extract_paper_extractions.py`

核心策略是：**优先结构化来源（XML 表格、补充表格）**，并把每条结果都做成“可追溯记录”。

1) 表格重建（用于复核）：
   - 从 JATS XML 提取表格，重建为 `extracted_tables/*.csv`
   - 从补充材料（xlsx/csv/tsv/zip/tar）中解析表格并落盘为 CSV
2) 原始抽取（`raw_extractions.csv`）：
   - 序列片段（sequence）
   - 端点数值（endpoint_value）
   - 条件词（condition）
   - 阈值句（threshold_text）
   - 每条都带 `source_path + source_ref` 指针（表格 id / 行列 / 文件名 / 页码等）
3) 行级 JOIN 抽取（`raw_experimental_records.csv`）：
   - 先在所有表格中建立 `peptide_id → sequence` 的映射（“序列表”）
   - 再在其它表格中识别端点列与数值列，把 `peptide_id` 与端点表 join 到一起
   - 支持若干常见非标准布局：
     - **转置表**（peptide 在列、菌株/条件在行）
     - `MIC/MBC` 的 `4(8)` 与 `2/2` 形式自动拆分为两条记录
     - id 列别名（`Agents/Treatments/No./Abbrev` 等）
   - 同时过滤明显“派生指标列”（如 `SI=CC50/IC50`）避免把派生值当原始测量

输出产物：

- `<papers>/raw_experimental_records.csv`
- `<papers>/raw_extractions.csv`
- `<papers>/extracted_tables/*.csv`

#### Step D：按“可比性/训练可用性”再筛查（QC + 标记）

脚本：`scripts/screen_experimental_records.py`

目的：把“抽到了记录”进一步分层为：

- **严格可比（train_ready_strict）**：单位齐全 + 非汇总/派生 + 非删失（无 `>`/`<`）+ AA20 标准序列
- **宽松可用（train_ready_relaxed）**：允许删失值（例如 `>128`）但仍要求“有单位 + 非汇总/派生”（适合做删失/区间建模）

输出：

- `<papers>/screened_experimental_records.csv`：在原始记录基础上新增 QC 列（`unit_missing/is_censored/is_aggregate/seq_is_canonical20/train_ready_*` 等）
- `<papers>/screening_report.md`：按论文汇总的 QC 报告（含每篇 strict/relaxed 统计、问题分布、0-record 列表）

### 8.4 本次实际运行（目录与命令）

本次收集已固化为两个可复现 run：

- OA10（已下载 10 篇并反复迭代抽取规则）：
  - `runs/oa10_toxicity_seq/papers`
  - `runs/oa10_toxicity_seq/papers/extraction_report.md`
  - `runs/oa10_toxicity_seq/papers/screening_report.md`
- OA50（候选 50 篇 OA 的批量下载+抽取）：
  - `runs/oa50_toxicity_seq/papers`
  - `runs/oa50_toxicity_seq/papers/extraction_report.md`
  - `runs/oa50_toxicity_seq/papers/screening_report.md`

如需复跑同样流程，推荐用编排脚本（会串起下载+抽取+报告）：

```bash
python scripts/run_tox_paper_pipeline.py \
  --paper-ids runs/oa50_candidate_ids.txt \
  --run-name oa50_toxicity_seq \
  --max-pdf-pages 8 \
  --download-timeout 45 \
  --download-delay 0.4 \
  --max-file-mb 600 \
  --max-file-seconds 300
```

然后对产物做 QC：

```bash
python scripts/screen_experimental_records.py --input-dir runs/oa50_toxicity_seq/papers
```

### 8.5 为什么还需要“再筛查”

同名端点在不同论文中实验体系可能不一致（菌株/细胞系/培养条件/读出方法不同），因此需要把记录按“是否具备可比性”分层：

- 对“跨论文 benchmark/可比对”的训练，优先用 `train_ready_strict=1`
- 对“最大化数据规模”的训练，使用 `train_ready_relaxed=1` 并显式处理删失/区间值
- 对“聚合行/派生指标”（GM、Gram+/Gram-、SI/TI 等），建议单独保留但不要混进原始端点监督信号

### 8.6 可复核性：如何从一条记录追到原始表格

每条 `raw_experimental_records.csv` 都有 `source_ref`，形如：

- `table_id=...;row_idx=...;col_idx=...;header_row_idx=...`

复核路径：

1) 找到对应 `paper_stem`（同名 `.xml` 在 `<papers>/` 下）
2) 打开 `<papers>/extracted_tables/<stem>__xml__<table_id>.csv`（或补充材料生成的表 CSV）
3) 根据 `row_idx/col_idx` 定位单元格，核对 `peptide_id/sequence/endpoint/value/unit/condition`

### 8.7 已知局限（当前策略有意为之）

- **图中数据（plot）不做数值数字化**：只要作者提供了补充表格，就优先用补充表格；否则建议人工补录或后续再做专门的图像数字化模块。
- 单位缺失/条件缺失时会降低 `train_ready_strict`；这类记录仍保留在 raw 表中，方便后续人工补全或规则增强。

---

## 9. 全面下载方案（“全网覆盖”的合规路径，更新时间：2026-01-17 12:09 +0800）

你现在的 OA 下载链路已经跑通，但如果目标是“尽可能全面”地把相关论文 PDF/补充材料收集到本地，建议采用 **OA 优先 + 订阅/学校权限补齐 + 手工/馆际/作者补齐** 的三段式方案，并在流程里显式记录“版权/许可/来源”，避免后续训练数据的合规风险。

### 9.1 关键结论：学校账号能不能用于脚本化下载？

通常是：

- 你能“网页登录阅读/下载单篇 PDF”并不等于你能“**批量脚本化抓取**”。
- 是否允许自动化下载/文本挖掘（TDM）取决于：**学校与出版社的许可条款 + 你所在网络环境 + 出版社的技术通道**。

更稳妥的做法是：把“订阅内容”纳入一个 **TDM 合规通道**（官方 API / 官方 TDM token / 白名单 IP / 厂商批量数据服务），而不是写爬虫去模拟登录或抓网页。

你需要向图书馆/信息中心确认（直接问这几条）：

1) 我校对目标出版社是否有 **Text and Data Mining (TDM)** 权利条款？
2) 是否提供 **EZproxy / VPN / Shibboleth / OpenAthens** 等校外访问方式？自动化访问是否允许？
3) 是否能为研究用途申请 **出版社官方 TDM API key / token**（或由学校统一申请并提供）？
4) 是否有并发/速率/下载量限制？是否允许长期缓存到本地用于科研（以及是否允许用于模型训练）？

### 9.2 “尽可能全面”的下载分层（推荐顺序）

#### Layer 0：先统一建一个“全局去重清单”（避免重复下载/重复计算）

无论你启多少个 Codex 进程，第一步都建议先生成一个全局 ids 列表（优先 DOI，其次 PMCID/PMID），并去重：

- `all_ids.txt`：一行一个 DOI/PMCID/PMID（去重）
- `library.sqlite`（可选）：记录每篇的下载状态、来源、sha256、许可、版本（preprint/published）等

这样后续“下载”和“抽取”都可以分离：下载做一次，抽取可以并行多次且不重复。

#### Layer 1：OA-only 自动下载（你现在的 pipeline）

目标：把能合法公开获取的版本尽可能拿全（PDF + XML + 补充材料 + OA 包）。

推荐通道（优先级从高到低）：

1) **PMC OA package / Europe PMC fullTextXML**（可拿 XML，且经常能带补充材料）
2) **Unpaywall**（OA PDF 解析）
3) **OpenAlex**（OA PDF 链接/落地页）
4) **预印本与开放仓库**：arXiv / bioRxiv / medRxiv / Zenodo / Figshare / Dryad（常见存放补充表、source data）
5) **期刊自身的 OA PDF**（PLOS、BMC、Frontiers、MDPI 等）

落地建议：

- 继续用本仓库脚本：`scripts/fetch_open_access_papers.py`（并保留 `download_manifest.jsonl` 做可审计 provenance）
- 对于“source data”类补充材料：优先解析 XML 中 `<supplementary-material>` 链接与 PMC OA 包解包结果

#### Layer 2：出版社官方 TDM / API（订阅内容的“合规批量下载”）

目标：对“非 OA 但学校订阅可访问”的论文，走官方批量通道（通常需要 token / API key）。

通用做法：

1) 用 DOI 列表向出版社/聚合 API 解析“full-text 可用性/下载端点/许可”。
2) 用官方 TDM 端点下载全文（可能是 XML/HTML、也可能是 PDF 或打包的全文集）。
3) 严格遵守速率限制、并发限制、用途限制；把许可信息写入 manifest。

注意：

- 不同出版社对“可下载格式”差异很大：有的给全文 XML/HTML，PDF 可能不开放给脚本；有的要求在校 IP；有的要求额外签 TDM 协议。
- 这一步最可能需要你提供 **API key / token / 白名单 IP / 校园 VPN**（取决于学校与出版社）。

我建议你先列出你最常遇到的出版社（例如 Elsevier / Springer Nature / Wiley / ACS / RSC / IEEE / OUP / Cambridge / Nature/Science 系列等），然后：

- 去图书馆确认是否已有统一的 TDM 入口或可申请 token
- 选 1–2 家出版社做“试点”，把 token 流程跑通，再扩展

#### Layer 3：人工补齐（仍然合规，但不自动化）

目标：当 Layer 2 走不通时，仍能把关键论文纳入语料库。

策略：

- 用学校账号在浏览器/校园网/VPN 下逐篇下载 PDF（或通过图书馆“全文传递/馆际互借”）
- 把下载的 PDF 放入统一目录（例如 `papers_subscription/manual_import/`），同时补一份元数据（DOI/标题/来源/日期/许可条款链接）
- 后续抽取脚本只读取本地文件，不做任何“登录抓取”

### 9.3 多进程并行时如何避免“下载重复/互相覆盖”

结论：**下载阶段不建议多进程同时写同一个目录**；抽取阶段可以并行。

推荐两种模式：

- 模式 A（最稳）：**先单进程下载 → 多进程抽取**
  - 下载只跑一次，目录作为只读语料库
  - 抽取可以按 paper 分片并行（不同进程输出到不同目录/不同 CSV，最后合并）
- 模式 B（必须并发下载时）：给 downloader 加全局锁
  - 用 `flock`（或同类文件锁）保证同一时刻只有一个进程在写 `download_manifest.jsonl` 与 `.part` 临时文件

当前仓库的 downloader 使用固定的 `.part` 临时文件名与 JSONL 追加写 manifest；多进程并发时存在互相覆盖/manifest 交错的风险，因此建议按模式 A 组织。

### 9.4 “足够全面”的补充：不只下载 PDF，还要下载补充材料/源数据

对你要做的“序列 + 实验数值”而言，最有价值的往往不是主文 PDF，而是：

- Supplementary Tables（xlsx/csv）
- Source Data（csv/xlsx，Nature/Science 系列常见）
- 机构仓库（Zenodo/Figshare/Dryad）的原始表格

因此建议下载侧的“完成定义”是：

- 每篇至少拿到：`XML`（用于提取表格）或可机读补充表
- 同时尽量拿到：supplementary/source data（决定你能抽到多少一手数据）

### 9.5 对后续模型训练的合规与可比性提醒（强烈建议）

- 把数据分为 `oa/` 与 `subscription/` 两个语料库目录，并在 manifest 里记录 license/usage（能否用于训练、能否再分发）。
- 训练时优先用 `train_ready_strict=1` 的子集做跨论文 benchmark；订阅内容如果许可不允许模型训练，应从训练集中剔除或仅用于人工阅读/假设生成。

如果你告诉我：学校的校外访问方式（VPN/EZproxy/OpenAthens 等）以及你最想覆盖的出版社列表，我可以把 Layer 2（官方 TDM/API）的“token/接口/落地文件格式/限速策略”进一步做成可执行的扩展脚本与 skill 流程（仍保持合规，不做登录模拟抓取）。 

---

## 10. OA 2000 批量覆盖（AMP+毒素，“都要”，更新时间：2026-01-17 12:09 +0800）

目标：在 **OA-only** 前提下，尽可能覆盖 **2000 篇**与“肽/蛋白序列 + 毒性/溶血/抗菌端点（MIC/MBC/IC50/CC50/HC10/HC50/MHC/LD50/LC50）”相关的文献，并把可抽取的一手数据落地为可审计的表格与记录。

### 10.1 运行策略（两段式，先快后全）

为避免“大规模 discovery 阶段重复下载 XML”，本次采用：

1) **快速发现**（不下载 fulltext XML，仅用 Europe PMC 搜索结果筛出 OA+inPMC 的 PMCID 列表）  
2) **统一下载**（下载一次 XML；后续再按需补齐 PDF/补充材料）  
3) **抽取 + 报告 + QC**（得到 `raw_experimental_records.csv` 与 `train_ready_*` 标记）

这样可以：
- 降低网络请求量（避免 discovery 把 XML 下载一遍，download 再下载一遍）
- 更适合大规模（2000+）跑批

### 10.2 检索 query（Europe PMC）

本次选择“AMP + 毒素都要”的合并 query（偏向一手实验端点与序列表述）：

```text
(((peptide OR peptides OR "antimicrobial peptide" OR AMP OR toxin OR venom OR "protein toxin")
  AND (MIC OR MBC OR "minimum inhibitory concentration" OR "minimum bactericidal concentration"
       OR IC50 OR CC50 OR EC50 OR HC10 OR HC50 OR MHC OR hemolysis OR hemolytic OR LD50 OR LC50
       OR toxicity OR cytotoxicity OR cytotoxic)
  AND (sequence OR sequences OR "amino acid sequence"))
 NOT PUB_TYPE:"Review")
```

### 10.3 具体执行命令（可复现）

运行目录（本次批量 run）：建议使用时间戳命名，例如：

- `runs/oa2000_amp_tox_20260117_1209/`

#### Step A：快速发现 2000 篇（输出 PMCID 列表）

脚本：`scripts/discover_epmc_pmcids_fast.py`

```bash
mkdir -p runs/oa2000_amp_tox_20260117_1209

python scripts/discover_epmc_pmcids_fast.py \
  --query '(((peptide OR peptides OR \"antimicrobial peptide\" OR AMP OR toxin OR venom OR \"protein toxin\") AND (MIC OR MBC OR \"minimum inhibitory concentration\" OR \"minimum bactericidal concentration\" OR IC50 OR CC50 OR EC50 OR HC10 OR HC50 OR MHC OR hemolysis OR hemolytic OR LD50 OR LC50 OR toxicity OR cytotoxicity OR cytotoxic) AND (sequence OR sequences OR \"amino acid sequence\")) NOT PUB_TYPE:\"Review\")' \
  --max-results 20000 \
  --keep 2000 \
  --out runs/oa2000_amp_tox_20260117_1209/candidates.csv \
  --ids-out runs/oa2000_amp_tox_20260117_1209/paper_ids.txt
```

产物：
- `runs/oa2000_amp_tox_20260117_1209/candidates.csv`：候选清单（含 hasSuppl/hasPDF 等提示）
- `runs/oa2000_amp_tox_20260117_1209/paper_ids.txt`：2000 个 PMCID（去重）

#### Step B：下载（先 XML 为主；后续按需补齐 supplementary）

先下载 XML（快速、体积小；足够支撑主文表格抽取）：

```bash
python scripts/fetch_open_access_papers.py \
  --input runs/oa2000_amp_tox_20260117_1209/paper_ids.txt \
  --out runs/oa2000_amp_tox_20260117_1209/papers \
  --format xml \
  --timeout 45 \
  --delay 0.15
```

（可选）如果你希望进一步提高“一手实验数据”覆盖率，再对 subset 追加补充材料下载（会更慢、更占空间）：
- 从 `candidates.csv` 里筛 `hasSuppl==Y` 的 PMCID 形成 `paper_ids_has_suppl.txt`
- 对该 subset 再跑一次：

```bash
python scripts/fetch_open_access_papers.py \
  --input runs/oa2000_amp_tox_20260117_1209/paper_ids_has_suppl.txt \
  --out runs/oa2000_amp_tox_20260117_1209/papers \
  --format xml \
  --supplementary \
  --timeout 45 \
  --delay 0.2 \
  --max-file-mb 600 \
  --max-file-seconds 300
```

#### Step C：抽取 + 报告 + QC

```bash
python scripts/extract_paper_extractions.py \
  --input-dir runs/oa2000_amp_tox_20260117_1209/papers \
  --overwrite \
  --no-pdf

python scripts/make_extraction_report.py --input-dir runs/oa2000_amp_tox_20260117_1209/papers
python scripts/screen_experimental_records.py --input-dir runs/oa2000_amp_tox_20260117_1209/papers
```

重点产物：
- `runs/oa2000_amp_tox_20260117_1209/papers/download_manifest.jsonl`
- `runs/oa2000_amp_tox_20260117_1209/papers/raw_experimental_records.csv`
- `runs/oa2000_amp_tox_20260117_1209/papers/extraction_report.md`
- `runs/oa2000_amp_tox_20260117_1209/papers/screened_experimental_records.csv`
- `runs/oa2000_amp_tox_20260117_1209/papers/screening_report.md`

### 10.4 预计耗时与注意事项

- 2000 篇规模下载会显著耗时；建议“下载单进程 + 抽取可并行”的组织方式（见 9.3）。
- 如果中途中断，可重复运行 download/extract（已下载会 `skipped_exists`；manifest 保留过程记录）。
